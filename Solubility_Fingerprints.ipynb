{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solubility_Fingerprints.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ojfWMzcjnZ_JhTjdlmO7Lcz7bF0hX-Ae",
      "authorship_tag": "ABX9TyMnqwIzqwdaaz3KRmrd61ey",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ali1810/Deep_Chem/blob/main/Solubility_Fingerprints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtDhZEHpqlZ3"
      },
      "source": [
        "### Installation of condacolab to install rdkit ....\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBtajV7XqhE5"
      },
      "source": [
        "### Installing RDKIT \n",
        "\n",
        "!mamba install -c conda-forge rdkit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4olPPSjq037K"
      },
      "source": [
        "import pandas as pd \n",
        "import keras as k\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, BatchNormalization, Input, Dense\n",
        "from keras.layers.core import Dense\n",
        "#from keras.optimizer import Adam, SGD\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras import losses\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import rdMolDescriptors, Draw\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem.Fingerprints import FingerprintMols\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "from numpy import ndarray\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import re\n",
        "import string\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import csv\n",
        "from collections import defaultdict\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc2l2xAFHMea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6a65bd3f-79b3-4770-9963-af8a90c5225d"
      },
      "source": [
        "data=pd.read_csv('/content/drive/MyDrive/KIT/delaney.csv')\n",
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Compound ID</th>\n",
              "      <th>measured log(solubility:mol/L)</th>\n",
              "      <th>ESOL predicted log(solubility:mol/L)</th>\n",
              "      <th>SMILES</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1,1,1,2-Tetrachloroethane</td>\n",
              "      <td>-2.18</td>\n",
              "      <td>-2.794</td>\n",
              "      <td>ClCC(Cl)(Cl)Cl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1,1,1-Trichloroethane</td>\n",
              "      <td>-2.00</td>\n",
              "      <td>-2.232</td>\n",
              "      <td>CC(Cl)(Cl)Cl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1,1,2,2-Tetrachloroethane</td>\n",
              "      <td>-1.74</td>\n",
              "      <td>-2.549</td>\n",
              "      <td>ClC(Cl)C(Cl)Cl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1,1,2-Trichloroethane</td>\n",
              "      <td>-1.48</td>\n",
              "      <td>-1.961</td>\n",
              "      <td>ClCC(Cl)Cl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1,1,2-Trichlorotrifluoroethane</td>\n",
              "      <td>-3.04</td>\n",
              "      <td>-3.077</td>\n",
              "      <td>FC(F)(Cl)C(F)(Cl)Cl</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Compound ID  ...               SMILES\n",
              "0       1,1,1,2-Tetrachloroethane  ...       ClCC(Cl)(Cl)Cl\n",
              "1           1,1,1-Trichloroethane  ...         CC(Cl)(Cl)Cl\n",
              "2       1,1,2,2-Tetrachloroethane  ...       ClC(Cl)C(Cl)Cl\n",
              "3           1,1,2-Trichloroethane  ...           ClCC(Cl)Cl\n",
              "4  1,1,2-Trichlorotrifluoroethane  ...  FC(F)(Cl)C(F)(Cl)Cl\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBneEw7HIBSy"
      },
      "source": [
        "smiles_array=data.SMILES"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gndzr1ZlXO_U"
      },
      "source": [
        "data[\"smiles_length\"]= data[\"SMILES\"].apply(len)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ktXa3THIHua"
      },
      "source": [
        "solubility_array= data.iloc[:,2]\n",
        "#print(solubility_array)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swLSgbS4IM7C",
        "outputId": "80d8c5b8-67ff-439e-db13-fd14857074a4"
      },
      "source": [
        "print(len(smiles_array))\n",
        "print(len(solubility_array))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1144\n",
            "1144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACHt7uGtIk0i"
      },
      "source": [
        "#Turning SMILES into Explicit Bit Vectors (RDKit prefered format)\n",
        "mols = [Chem.rdmolfiles.MolFromSmiles(SMILES_string) for SMILES_string in smiles_array]\n",
        "\n",
        "\n",
        "#Convert training molecules into training fingerprints\n",
        "bi = {}\n",
        "fingerprints = [Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(m, radius=2, bitInfo= bi, nBits=512) for m in mols]\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeFfw-U8JBBr"
      },
      "source": [
        "#Convert training fingerprints into binary, and put all training binaries into arrays\n",
        "import numpy as np \n",
        "\n",
        "fingerprints_array = []\n",
        "for fingerprint in fingerprints:\n",
        "        array = np.zeros((1,), dtype= int)\n",
        "        DataStructs.ConvertToNumpyArray(fingerprint, array)\n",
        "        fingerprints_array.append(array)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53S1NZF1QRBR"
      },
      "source": [
        "fingerprints_array =np.array(fingerprints_array)\n",
        "solubility_array =np.array(solubility_array)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xm0tqZKJK2M",
        "outputId": "1dccea89-80be-40c3-c941-ea60efeb7cff"
      },
      "source": [
        "print(len(fingerprints_array))\n",
        "print(len(solubility_array))\n",
        "print(fingerprints_array.shape)\n",
        "print(solubility_array.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1144\n",
            "1144\n",
            "(1144, 512)\n",
            "(1144,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1WtztOSQS5-"
      },
      "source": [
        "### Write a function to calculate these values....\n",
        "\n",
        "def getAromaticProportion(m):\n",
        "    aromatic_list = [m.GetAtomWithIdx(i).GetIsAromatic() for i in range(m.GetNumAtoms())]\n",
        "    aromatic = 0\n",
        "    for i in aromatic_list:\n",
        "        if i:\n",
        "            aromatic += 1\n",
        "    heavy_atom = Lipinski.HeavyAtomCount(m)\n",
        "    return aromatic / heavy_atom\n",
        "\n",
        "\n",
        "def generate(smiles):\n",
        "    moldata = []\n",
        "    for elem in smiles:\n",
        "        mol = Chem.MolFromSmiles(elem)\n",
        "        moldata.append(mol)\n",
        "\n",
        "    baseData = np.arange(1, 1)\n",
        "    i = 0\n",
        "    for mol in moldata:\n",
        "\n",
        "        desc_MolLogP = Crippen.MolLogP(mol)\n",
        "        desc_MolWt = Descriptors.MolWt(mol)\n",
        "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
        "        desc_AromaticProportion = getAromaticProportion(mol)\n",
        "        desc_Ringcount        =   Descriptors.RingCount(mol)\n",
        "        desc_TPSA = Descriptors.TPSA(mol)\n",
        "        desc_Hdonrs=Lipinski.NumHDonors(mol)\n",
        "        desc_SaturatedRings = Lipinski.NumSaturatedRings(mol)   \n",
        "        desc_AliphaticRings = Lipinski.NumAliphaticRings(mol) \n",
        "        desc_HAcceptors = Lipinski.NumHAcceptors(mol)\n",
        "        desc_Heteroatoms = Lipinski.NumHeteroatoms(mol)\n",
        "        #desc_molMR=Descriptors.MolMR(mol)\n",
        "        row = np.array([desc_MolLogP,\n",
        "                        desc_MolWt,\n",
        "                        desc_NumRotatableBonds,\n",
        "                        desc_AromaticProportion,desc_Ringcount,desc_TPSA,desc_Hdonrs,desc_SaturatedRings,desc_AliphaticRings,desc_HAcceptors,desc_Heteroatoms])\n",
        "\n",
        "        if i == 0:\n",
        "            baseData = row\n",
        "        else:\n",
        "            baseData = np.vstack([baseData, row])\n",
        "        i = i + 1\n",
        "\n",
        "    columnNames = [\"MolLogP\", \"MolWt\", \"NumRotatableBonds\", \"AromaticProportion\",\"Ring_Count\",\"TPSA\",\"H_donors\",\"Saturated_Rings\",\"AliphaticRings\",\"H_Acceptors\",\"Heteroatoms\"]\n",
        "    descriptors = pd.DataFrame(data=baseData, columns=columnNames)\n",
        "\n",
        "    return descriptors\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOacIpWHQzIP"
      },
      "source": [
        "import numpy as np\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit.Chem import Lipinski\n",
        "from rdkit.Chem import Crippen"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujf2RS7_QOCL",
        "outputId": "18bc10da-ac83-4512-ae87-a280fd6aaad5"
      },
      "source": [
        "descriptors =generate(data.SMILES)\n",
        "print(descriptors)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      MolLogP    MolWt  ...  H_Acceptors  Heteroatoms\n",
            "0     2.59540  167.850  ...          0.0          4.0\n",
            "1     2.37650  133.405  ...          0.0          3.0\n",
            "2     2.59380  167.850  ...          0.0          4.0\n",
            "3     2.02890  133.405  ...          0.0          3.0\n",
            "4     2.91890  187.375  ...          0.0          6.0\n",
            "...       ...      ...  ...          ...          ...\n",
            "1139  1.98820  287.343  ...          6.0          8.0\n",
            "1140  3.42130  286.114  ...          3.0          6.0\n",
            "1141  3.60960  308.333  ...          4.0          4.0\n",
            "1142  2.56214  354.815  ...          4.0          8.0\n",
            "1143  2.02164  179.219  ...          2.0          3.0\n",
            "\n",
            "[1144 rows x 11 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyEJ4fnHRUc5"
      },
      "source": [
        "import numpy as np \n",
        "df1=np.array(descriptors)\n",
        "\n",
        "#df_new=fingerprints_array.join(df1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaYtt-8VJ1DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584d7127-1342-43fe-de84-8a443537e0f9"
      },
      "source": [
        "df1.shape\n",
        "fingerprints_array.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1144, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIv3BcZiTmgo",
        "outputId": "088eb755-7759-4064-c697-2b1e0a9773f7"
      },
      "source": [
        "### Adding two numpy array with column wise .....\n",
        "df_new = np.append(fingerprints_array, df1, axis = 1)\n",
        "df_new.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1144, 523)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWFdRp5_Jig2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO0_iEVhJt7L"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_new, solubility_array, test_size = 0.2)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKu3T88TaOXc",
        "outputId": "95f389a4-0458-474d-ab5a-a2886fdebe77"
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.22206289 -0.45335813 -0.16760038 -0.12465268 -0.22206289 -0.09966832\n",
            " -0.09966832 -0.24042352 -0.19644976 10.64776972 -0.13340747 -0.18726418\n",
            " -0.08780234 -0.22475909 -0.15695878 -0.34037282 -0.15695878 -0.18411492\n",
            " -0.18726418 -0.17767127 -0.13758982 -0.12005173 -0.11527808 -0.13758982\n",
            " -0.15695878 -0.16412199 -0.09391638 -0.10511767 -0.24042352 -0.14562059\n",
            "  0.         -0.14165765 -0.10511767 -1.34415344 -0.17101563 -0.16760038\n",
            " -0.4051199  -0.23786968 -0.16057632 -0.14948702 -0.11527808 -0.09391638\n",
            " -0.14948702 -0.18091815 -0.15695878 -0.18726418 -0.24295633 -0.16760038\n",
            " -0.05735393 -0.47250612 -0.23007479 -0.12005173 -0.14948702 -0.20815747\n",
            " -0.08124445 -0.11527808 -0.12909944 -0.13758982 -0.13758982 -0.09391638\n",
            " -0.04680364 -0.17101563 -0.16412199 -0.25532741  1.40728676 -0.12005173\n",
            " -0.31164451 -0.17767127 -0.17767127 -0.16057632 -0.1103093  -0.25043516\n",
            " -0.14948702 -0.21099893 -0.15695878 -0.14165765 -0.1903683  -0.15326426\n",
            " -0.14562059 -0.34633949 -0.9030372  -0.16412199 -0.05735393 -0.10511767\n",
            " -0.22206289 -0.24796153 -0.17101563 -0.12909944 -0.22475909 -0.16412199\n",
            " -0.36390348 -0.12909944 -0.12909944 -0.08124445 -0.13758982 -0.18091815\n",
            " -0.13758982 -0.21658891 -0.12005173 -0.05735393 -0.08124445 -0.16760038\n",
            " -0.15695878 -0.09966832 -0.13758982  7.26797973 -0.13758982 -0.10511767\n",
            " -0.06626296 -0.21380899 -0.21933984 -0.14165765 -0.16412199 -0.15695878\n",
            " -0.29663191 -0.09391638 -0.24042352 -0.19644976 -0.16760038 -0.26253616\n",
            " -0.11527808 -0.21380899 -0.04680364 -0.08780234 -0.15326426 -0.09966832\n",
            " -0.12005173 -0.13340747 -0.4051199  -0.18091815 -0.22742941 -0.11527808\n",
            " -0.03307706 -0.15695878 -0.13340747 -0.13340747 -0.18726418 -0.14165765\n",
            "  1.05506586 -0.23007479 -0.18091815 -0.08124445 -0.12005173 -0.09966832\n",
            " -0.32620347 -0.17101563 -0.12909944 -0.71756579 -0.07412493 -0.15326426\n",
            " -0.09391638 -0.20815747 -0.1103093  -0.06626296 -0.18411492 -0.13758982\n",
            " -0.09391638 -0.15326426 -0.14165765 -0.09966832 -0.17437146 -0.1103093\n",
            " -0.22206289 -0.18091815 -0.16057632 -0.27653316 -0.08124445 -0.20528329\n",
            " -0.16760038 -0.08780234 -0.11527808 -0.24295633 -0.15695878 -0.09966832\n",
            " -0.07412493  1.65369099 -0.12465268 -0.07412493 -0.12465268 -0.20237498\n",
            " -0.22206289 -0.16057632 -0.34831527 -0.54240285 -0.16412199 -0.13758982\n",
            " -0.40694606 -0.1903683  -0.12005173 -0.09391638 -0.19644976 -0.24796153\n",
            " -0.22206289 -0.30312611 -0.12909944 -0.19943101  0.         -0.13340747\n",
            " -0.11527808 -0.14948702 -0.09966832 -0.23269608 -0.29444581 -0.32825019\n",
            " -0.20237498 -0.14562059 -0.21933984 -0.11527808 -0.10511767 -0.16057632\n",
            " -0.11527808 -0.18726418 -0.18411492 -0.16760038  0.98373875 -0.11527808\n",
            " -0.37914377 -0.08780234 -0.18411492 -0.1103093  -0.1103093  -0.08780234\n",
            " -0.32620347 -0.06626296 -0.16412199 -0.13340747 -0.11527808 -0.26014985\n",
            " -0.17101563 -0.1103093  -0.09966832 -0.1903683  -0.1103093  -0.33434436\n",
            " -0.09966832 -0.17101563 -0.14948702 -0.14948702 -0.17101563  6.0930288\n",
            " -0.10511767 -0.25532741 -0.23007479 -0.24042352 -0.06626296 -0.23007479\n",
            " -0.1103093  -0.12465268 -0.15326426 -0.16760038 -0.34037282 -0.16057632\n",
            " -0.14948702 -0.08780234 -0.11527808 -0.1903683  -0.13340747 -0.1103093\n",
            " -0.16057632 -0.12005173 -0.12909944 -0.10511767 -0.17101563 -0.14165765\n",
            " -0.12005173 -0.1103093  -0.13758982 -0.11527808 -0.24295633 -0.12909944\n",
            " -0.13340747 -0.18726418 -0.14562059 -0.16412199 -0.12465268 -0.16412199\n",
            " -0.09391638 -0.09391638 -0.20237498 -0.12909944 -0.24042352 -0.06626296\n",
            " -0.43572578 -0.38102687 -0.12465268 -0.1903683  -0.17767127 -0.23007479\n",
            " -0.15326426 -0.2577471  -0.14562059 -0.14562059 -0.1103093  -0.12465268\n",
            " -0.67430704  1.12965455 -0.10511767 -0.14948702 -0.08780234 -0.12465268\n",
            " -0.08124445 -0.29663191 -0.12909944 -0.08124445 -0.29663191 -0.12005173\n",
            " -0.05735393 -0.08780234 -0.14948702 -0.1103093  -0.15695878 -0.19342949\n",
            " -0.17767127 -0.12005173 -0.48113818 -0.14948702 -0.1103093  -0.16057632\n",
            " -0.10511767 -0.37725601 -0.15695878 -0.1903683  -0.27192584 -0.18091815\n",
            " -0.16412199 -0.16057632 -0.2577471  -0.18726418 -0.13340747 -0.35028472\n",
            " -0.24295633 -0.24042352 -0.17101563 -0.26490647 -0.17767127 -0.08780234\n",
            " -0.12005173  0.76716469 -0.15695878 -0.07412493 -0.12465268 -0.21099893\n",
            " -0.23269608 -0.16760038 -0.08124445 -0.18726418 -0.1103093  -0.09966832\n",
            " -0.19644976 -0.14562059 -0.21658891 -0.08780234 -0.28108617 -0.16412199\n",
            " -0.18091815 -0.09391638  0.66916725 -0.09966832 -0.1903683  -0.10511767\n",
            " -0.22206289  4.297451   -0.20815747 -0.51702323 -0.14165765 -0.07412493\n",
            " -0.32825019 -0.26960089 -0.09391638 -0.30527031 -0.11527808 -0.12005173\n",
            " -0.17437146 -0.14165765 -0.09391638 -0.36004115 -0.1903683  -0.09966832\n",
            " -0.43394895 -0.15326426 -0.12909944  1.71085903 -0.09391638 -0.12465268\n",
            " -0.18091815 -0.17437146 -0.23007479 -0.16412199 -0.07412493  2.52567461\n",
            " -0.11527808 -0.13340747 -0.34435724 -0.15695878 -0.17437146 -0.06626296\n",
            " -0.10511767 -0.15695878 -0.14562059 -0.07412493 -0.12909944 -0.18091815\n",
            " -0.18091815 -0.20528329 -0.15695878 -0.15695878 -0.17767127 -0.17101563\n",
            " -0.1903683  -0.07412493 -0.08780234 -0.06626296 -0.20237498 -0.10511767\n",
            " -0.44280744 -0.17767127 -0.11527808 -0.11527808 -0.12909944 -0.14948702\n",
            " -0.33434436 -0.1103093  -0.09966832 -0.3542051  -0.14165765 -0.11527808\n",
            " -0.17101563 -0.26960089 -0.36004115 -0.1903683  -0.13340747 -0.14948702\n",
            " -0.17101563 -0.21933984 -0.21380899 -0.16412199 -0.12465268 -0.09966832\n",
            " -0.21933984 -0.12909944 -0.21099893 -0.20237498 -0.16760038 -0.09966832\n",
            " -0.15695878 -0.07412493 -0.13758982 -0.08780234 -0.23269608  5.43139025\n",
            " -0.09966832 -0.11527808 -0.21380899 -0.15326426 -0.09966832 -0.20237498\n",
            " -0.14562059 -0.09966832 -0.12005173 -0.1103093  -0.13340747 -0.19943101\n",
            " -0.23786968 -0.23529412 -0.07412493 -0.10511767 -0.14948702 -0.07412493\n",
            " -0.14948702 -0.12909944 -0.09391638 -0.09966832 -0.24796153 -0.18726418\n",
            " -0.12909944 -0.14948702 -0.1103093  -0.06626296 -0.14165765 -0.18726418\n",
            " -0.07412493 -0.15326426 -0.15326426 -0.07412493 -0.13340747 -0.18091815\n",
            " -0.17101563 -0.1103093  -0.14562059 -0.18411492 -0.09391638 -0.09391638\n",
            " -0.21658891 -0.19342949 -0.13340747 -0.10511767 -0.14562059 -0.18726418\n",
            " -0.14948702 -0.23007479 -0.10511767 -0.18091815 -0.14165765 -0.16412199\n",
            " -0.22206289 -0.27881628 -0.09966832 -0.38664905 -0.12005173 -0.03307706\n",
            " -0.13758982 -0.19943101  0.27765064 -0.20354042 -0.08825298  1.43088193\n",
            "  0.50470937 -0.53481787 -0.66636728 -0.36251983 -0.42400625 -0.54456362\n",
            " -0.86377639]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CWMjOhCTT8C"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjPEVKuhy6YZ"
      },
      "source": [
        "#### Applying different Machine learning and Deep learning techique to know which works best ....\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUy_GThbzzMr",
        "outputId": "3fc2de53-c56d-434f-933e-aa415e574eb7"
      },
      "source": [
        "model_svr=SVR(kernel='rbf')\n",
        "model_svr.fit(X_train, y_train)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
              "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RWazIyVzzC5",
        "outputId": "0b8b1dbf-6a94-4f02-d24c-7087ee78d535"
      },
      "source": [
        "model_rf = RandomForestRegressor(n_estimators = 10000, random_state = 0)\n",
        "model_rf.fit(X_train, y_train)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                      max_samples=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                      n_estimators=10000, n_jobs=None, oob_score=False,\n",
              "                      random_state=0, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5KfX1vqzywA",
        "outputId": "c3d48e13-f785-49ab-d895-84c6f7f503a5"
      },
      "source": [
        "model_xgr = XGBRegressor(n_estimators = 10000, random_state = 0)\n",
        "model_xgr.fit(X_train, y_train)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09:41:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
              "             max_depth=3, min_child_weight=1, missing=None, n_estimators=10000,\n",
              "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
              "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "             silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "SsUbYPz50XrW",
        "outputId": "21c7f850-b497-49a8-9aab-5cee423359c5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "preds = model_rf.predict(X_train)\n",
        "x_y_line = np.linspace(min(y_train.flatten()), max(y_train.flatten()), 500)\n",
        "plt.plot(y_train.flatten(), preds.flatten(), 'o', label='predictions')\n",
        "plt.plot(x_y_line, x_y_line, label='y=x')\n",
        "plt.xlabel(\"Log solubility (ground truth)\")\n",
        "plt.ylabel(\"Log solubility (predicted)\")\n",
        "plt.title(\"Parity plot: predictions vs ground truth data\")\n",
        "plt.legend()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f22eb2b9450>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx1eIUcp0rDM"
      },
      "source": [
        "Y_pred_test_rf = model_rf.predict(X_test)\n",
        "res = \"\\n\".join(\"{} {}\".format(x, y) for x, y in zip(y_test,Y_pred_test_rf ))\n",
        "#print(res)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xPRaVmV015K",
        "outputId": "1feb4316-eb5e-402f-c5ba-633a02e72035"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, Y_pred_test_rf)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9575759136375066"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o62XbAdf1Ard"
      },
      "source": [
        "Y_pred_test_xg = model_xgr.predict(X_test)\n",
        "res = \"\\n\".join(\"{} {}\".format(x, y) for x, y in zip(y_test,Y_pred_test_rf ))\n",
        "#print(res)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcoGGm671Mg6",
        "outputId": "b9a1915a-446f-46bd-a435-7e60c0b1dc11"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, Y_pred_test_xg)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9683027311311222"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38_ZJG9_2U-P"
      },
      "source": [
        "Y_pred_test_sv = model_svr.predict(X_test)\n",
        "res = \"\\n\".join(\"{} {}\".format(x, y) for x, y in zip(y_test,Y_pred_test_rf ))\n",
        "#print(res)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBM9n-Jo2iRc",
        "outputId": "e8a980cd-6c30-403c-c2a3-8b9970ce3a92"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, Y_pred_test_sv)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7413327698742944"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "LK2nZmzE3NIN",
        "outputId": "852919db-9442-40ec-ddf3-d041febd2d3a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "preds = model_xgr.predict(X_test)\n",
        "x_y_line = np.linspace(min(y_test.flatten()), max(y_test.flatten()), 500)\n",
        "plt.plot(y_test.flatten(), preds.flatten(), 'o', label='predictions')\n",
        "plt.plot(x_y_line, x_y_line, label='y_test')\n",
        "plt.xlabel(\"Log solubility (ground truth)\")\n",
        "plt.ylabel(\"Log solubility (predicted)\")\n",
        "plt.title(\"Parity plot: predictions vs ground truth data\")\n",
        "plt.legend()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f22eb00ac90>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0cKxo3pi2mk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwz0GK-Y8OX5"
      },
      "source": [
        "# Initialising the ANN\n",
        "model = Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "model.add(Dense(523, activation = 'relu', input_dim = 523))\n",
        "\n",
        "# Adding the second hidden layer\n",
        "model.add(Dense(units = 256, activation = 'relu'))\n",
        "\n",
        "# Adding the third hidden layer\n",
        "model.add(Dense(units = 128, activation = 'relu'))\n",
        "\n",
        "# Adding the fourth hidden layer\n",
        "model.add(Dense(units = 64, activation = 'relu'))\n",
        "\n",
        "# Adding the fourth hidden layer\n",
        "model.add(Dense(units = 32, activation = 'relu'))\n",
        "\n",
        "# Adding the fourth hidden layer\n",
        "model.add(Dense(units = 16, activation = 'relu'))\n",
        "\n",
        "# Adding the fourth hidden layer\n",
        "model.add(Dense(units = 8, activation = 'relu'))\n",
        "\n",
        "\n",
        "# Adding the output layer\n",
        "model.add(Dense(units = 1))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTbGM6EP8YYY"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error')"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwNkWAaZ8dxW",
        "outputId": "f4d55186-86a3-4ce1-80b8-6385f684e77b"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size = 10, epochs = 400)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "92/92 [==============================] - 1s 4ms/step - loss: 1.6637\n",
            "Epoch 2/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.4535\n",
            "Epoch 3/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2921\n",
            "Epoch 4/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1764\n",
            "Epoch 5/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1218\n",
            "Epoch 6/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0808\n",
            "Epoch 7/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0646\n",
            "Epoch 8/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0637\n",
            "Epoch 9/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0666\n",
            "Epoch 10/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0706\n",
            "Epoch 11/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0997\n",
            "Epoch 12/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0877\n",
            "Epoch 13/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0818\n",
            "Epoch 14/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0829\n",
            "Epoch 15/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0752\n",
            "Epoch 16/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0699\n",
            "Epoch 17/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0655\n",
            "Epoch 18/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0578\n",
            "Epoch 19/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0583\n",
            "Epoch 20/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0514\n",
            "Epoch 21/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0648\n",
            "Epoch 22/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0640\n",
            "Epoch 23/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0734\n",
            "Epoch 24/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0760\n",
            "Epoch 25/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1316\n",
            "Epoch 26/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0973\n",
            "Epoch 27/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0486\n",
            "Epoch 28/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0293\n",
            "Epoch 29/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0245\n",
            "Epoch 30/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0182\n",
            "Epoch 31/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0136\n",
            "Epoch 32/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0153\n",
            "Epoch 33/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0174\n",
            "Epoch 34/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0211\n",
            "Epoch 35/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0340\n",
            "Epoch 36/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0388\n",
            "Epoch 37/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0736\n",
            "Epoch 38/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0787\n",
            "Epoch 39/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0575\n",
            "Epoch 40/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0491\n",
            "Epoch 41/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0335\n",
            "Epoch 42/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0741\n",
            "Epoch 43/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0774\n",
            "Epoch 44/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0399\n",
            "Epoch 45/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0568\n",
            "Epoch 46/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0400\n",
            "Epoch 47/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0282\n",
            "Epoch 48/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0202\n",
            "Epoch 49/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0134\n",
            "Epoch 50/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0087\n",
            "Epoch 51/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0077\n",
            "Epoch 52/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0103\n",
            "Epoch 53/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0100\n",
            "Epoch 54/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0103\n",
            "Epoch 55/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0092\n",
            "Epoch 56/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0117\n",
            "Epoch 57/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0197\n",
            "Epoch 58/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0282\n",
            "Epoch 59/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0303\n",
            "Epoch 60/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0388\n",
            "Epoch 61/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0524\n",
            "Epoch 62/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0365\n",
            "Epoch 63/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0339\n",
            "Epoch 64/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0347\n",
            "Epoch 65/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0246\n",
            "Epoch 66/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0239\n",
            "Epoch 67/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0261\n",
            "Epoch 68/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0223\n",
            "Epoch 69/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0283\n",
            "Epoch 70/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0297\n",
            "Epoch 71/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0282\n",
            "Epoch 72/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0318\n",
            "Epoch 73/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0199\n",
            "Epoch 74/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0225\n",
            "Epoch 75/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0147\n",
            "Epoch 76/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0384\n",
            "Epoch 77/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0327\n",
            "Epoch 78/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0362\n",
            "Epoch 79/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0331\n",
            "Epoch 80/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0160\n",
            "Epoch 81/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0139\n",
            "Epoch 82/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0152\n",
            "Epoch 83/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0158\n",
            "Epoch 84/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0116\n",
            "Epoch 85/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0113\n",
            "Epoch 86/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0075\n",
            "Epoch 87/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0081\n",
            "Epoch 88/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0106\n",
            "Epoch 89/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0107\n",
            "Epoch 90/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0221\n",
            "Epoch 91/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0323\n",
            "Epoch 92/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0310\n",
            "Epoch 93/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0242\n",
            "Epoch 94/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0180\n",
            "Epoch 95/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0178\n",
            "Epoch 96/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0137\n",
            "Epoch 97/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0080\n",
            "Epoch 98/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0083\n",
            "Epoch 99/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0098\n",
            "Epoch 100/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0106\n",
            "Epoch 101/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0212\n",
            "Epoch 102/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0228\n",
            "Epoch 103/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0138\n",
            "Epoch 104/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0098\n",
            "Epoch 105/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0096\n",
            "Epoch 106/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0126\n",
            "Epoch 107/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0123\n",
            "Epoch 108/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0118\n",
            "Epoch 109/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0119\n",
            "Epoch 110/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0161\n",
            "Epoch 111/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0226\n",
            "Epoch 112/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0216\n",
            "Epoch 113/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0161\n",
            "Epoch 114/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0280\n",
            "Epoch 115/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0265\n",
            "Epoch 116/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0245\n",
            "Epoch 117/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0196\n",
            "Epoch 118/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0180\n",
            "Epoch 119/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0137\n",
            "Epoch 120/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0110\n",
            "Epoch 121/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0065\n",
            "Epoch 122/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0050\n",
            "Epoch 123/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0054\n",
            "Epoch 124/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0055\n",
            "Epoch 125/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0110\n",
            "Epoch 126/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0129\n",
            "Epoch 127/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0094\n",
            "Epoch 128/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0131\n",
            "Epoch 129/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0136\n",
            "Epoch 130/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0080\n",
            "Epoch 131/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0126\n",
            "Epoch 132/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0158\n",
            "Epoch 133/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0233\n",
            "Epoch 134/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0271\n",
            "Epoch 135/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0207\n",
            "Epoch 136/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0154\n",
            "Epoch 137/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0155\n",
            "Epoch 138/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0095\n",
            "Epoch 139/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0058\n",
            "Epoch 140/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0062\n",
            "Epoch 141/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0061\n",
            "Epoch 142/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0118\n",
            "Epoch 143/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0072\n",
            "Epoch 144/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0065\n",
            "Epoch 145/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0056\n",
            "Epoch 146/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0056\n",
            "Epoch 147/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0059\n",
            "Epoch 148/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0109\n",
            "Epoch 149/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0134\n",
            "Epoch 150/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0191\n",
            "Epoch 151/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0257\n",
            "Epoch 152/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0229\n",
            "Epoch 153/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0148\n",
            "Epoch 154/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0126\n",
            "Epoch 155/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0079\n",
            "Epoch 156/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0095\n",
            "Epoch 157/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0080\n",
            "Epoch 158/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0055\n",
            "Epoch 159/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0083\n",
            "Epoch 160/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0150\n",
            "Epoch 161/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0131\n",
            "Epoch 162/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0107\n",
            "Epoch 163/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0068\n",
            "Epoch 164/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0042\n",
            "Epoch 165/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0031\n",
            "Epoch 166/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 167/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0041\n",
            "Epoch 168/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 169/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0054\n",
            "Epoch 170/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0080\n",
            "Epoch 171/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0145\n",
            "Epoch 172/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0153\n",
            "Epoch 173/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0127\n",
            "Epoch 174/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0155\n",
            "Epoch 175/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0115\n",
            "Epoch 176/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0099\n",
            "Epoch 177/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0074\n",
            "Epoch 178/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0047\n",
            "Epoch 179/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0045\n",
            "Epoch 180/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 181/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 182/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 183/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 184/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0029\n",
            "Epoch 185/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0047\n",
            "Epoch 186/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0078\n",
            "Epoch 187/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0353\n",
            "Epoch 188/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0245\n",
            "Epoch 189/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0141\n",
            "Epoch 190/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0115\n",
            "Epoch 191/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0165\n",
            "Epoch 192/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0194\n",
            "Epoch 193/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0164\n",
            "Epoch 194/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0106\n",
            "Epoch 195/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0102\n",
            "Epoch 196/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0075\n",
            "Epoch 197/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0042\n",
            "Epoch 198/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0049\n",
            "Epoch 199/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0056\n",
            "Epoch 200/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0050\n",
            "Epoch 201/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0075\n",
            "Epoch 202/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0053\n",
            "Epoch 203/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0053\n",
            "Epoch 204/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0053\n",
            "Epoch 205/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 206/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0033\n",
            "Epoch 207/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 208/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 209/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0032\n",
            "Epoch 210/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 211/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0049\n",
            "Epoch 212/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0055\n",
            "Epoch 213/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0133\n",
            "Epoch 214/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0305\n",
            "Epoch 215/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0286\n",
            "Epoch 216/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0139\n",
            "Epoch 217/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0053\n",
            "Epoch 218/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0041\n",
            "Epoch 219/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0042\n",
            "Epoch 220/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 221/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 222/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 223/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 224/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 225/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 226/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 227/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 228/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0044\n",
            "Epoch 229/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0059\n",
            "Epoch 230/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0050\n",
            "Epoch 231/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0057\n",
            "Epoch 232/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0060\n",
            "Epoch 233/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0073\n",
            "Epoch 234/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0093\n",
            "Epoch 235/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0090\n",
            "Epoch 236/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0068\n",
            "Epoch 237/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0045\n",
            "Epoch 238/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0043\n",
            "Epoch 239/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0038\n",
            "Epoch 240/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0043\n",
            "Epoch 241/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0054\n",
            "Epoch 242/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0070\n",
            "Epoch 243/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0093\n",
            "Epoch 244/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0109\n",
            "Epoch 245/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0109\n",
            "Epoch 246/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0063\n",
            "Epoch 247/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0043\n",
            "Epoch 248/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0033\n",
            "Epoch 249/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0029\n",
            "Epoch 250/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 251/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 252/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 253/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 254/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0037\n",
            "Epoch 255/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0102\n",
            "Epoch 256/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0103\n",
            "Epoch 257/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0099\n",
            "Epoch 258/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0103\n",
            "Epoch 259/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0131\n",
            "Epoch 260/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0147\n",
            "Epoch 261/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0085\n",
            "Epoch 262/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0051\n",
            "Epoch 263/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0029\n",
            "Epoch 264/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 265/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0040\n",
            "Epoch 266/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0038\n",
            "Epoch 267/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0124\n",
            "Epoch 268/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0117\n",
            "Epoch 269/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0112\n",
            "Epoch 270/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0139\n",
            "Epoch 271/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0092\n",
            "Epoch 272/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0073\n",
            "Epoch 273/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0056\n",
            "Epoch 274/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0038\n",
            "Epoch 275/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0042\n",
            "Epoch 276/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0073\n",
            "Epoch 277/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0063\n",
            "Epoch 278/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0036\n",
            "Epoch 279/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 280/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 281/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 282/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 283/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 284/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 285/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0180\n",
            "Epoch 286/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0163\n",
            "Epoch 287/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0097\n",
            "Epoch 288/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0046\n",
            "Epoch 289/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 290/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 291/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 292/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0050\n",
            "Epoch 293/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0059\n",
            "Epoch 294/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 295/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0033\n",
            "Epoch 296/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0033\n",
            "Epoch 297/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 298/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 299/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 300/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 301/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 302/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 303/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0041\n",
            "Epoch 304/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0040\n",
            "Epoch 305/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0072\n",
            "Epoch 306/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0057\n",
            "Epoch 307/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0078\n",
            "Epoch 308/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0058\n",
            "Epoch 309/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0068\n",
            "Epoch 310/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0054\n",
            "Epoch 311/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0060\n",
            "Epoch 312/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0039\n",
            "Epoch 313/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0052\n",
            "Epoch 314/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0038\n",
            "Epoch 315/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 316/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 317/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0031\n",
            "Epoch 318/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0032\n",
            "Epoch 319/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 320/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0034\n",
            "Epoch 321/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0096\n",
            "Epoch 322/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0120\n",
            "Epoch 323/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0079\n",
            "Epoch 324/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0056\n",
            "Epoch 325/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0054\n",
            "Epoch 326/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0036\n",
            "Epoch 327/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 328/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 329/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 330/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 331/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 332/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 333/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0031\n",
            "Epoch 334/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0042\n",
            "Epoch 335/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0071\n",
            "Epoch 336/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0062\n",
            "Epoch 337/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0078\n",
            "Epoch 338/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0115\n",
            "Epoch 339/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0112\n",
            "Epoch 340/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0072\n",
            "Epoch 341/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0057\n",
            "Epoch 342/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0035\n",
            "Epoch 343/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 344/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 345/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 346/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 347/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 348/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 349/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0040\n",
            "Epoch 350/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0034\n",
            "Epoch 351/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 352/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 353/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 354/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 355/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0027\n",
            "Epoch 356/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0034\n",
            "Epoch 357/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0112\n",
            "Epoch 358/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0091\n",
            "Epoch 359/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0114\n",
            "Epoch 360/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0080\n",
            "Epoch 361/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0076\n",
            "Epoch 362/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0045\n",
            "Epoch 363/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0036\n",
            "Epoch 364/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0086\n",
            "Epoch 365/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0090\n",
            "Epoch 366/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0068\n",
            "Epoch 367/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0062\n",
            "Epoch 368/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0053\n",
            "Epoch 369/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0037\n",
            "Epoch 370/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0038\n",
            "Epoch 371/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0046\n",
            "Epoch 372/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0059\n",
            "Epoch 373/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0036\n",
            "Epoch 374/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 375/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 376/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 377/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 378/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0035\n",
            "Epoch 379/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 380/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0052\n",
            "Epoch 381/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0046\n",
            "Epoch 382/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 383/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 384/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0040\n",
            "Epoch 385/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 386/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0020\n",
            "Epoch 387/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 388/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0042\n",
            "Epoch 389/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0036\n",
            "Epoch 390/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0029\n",
            "Epoch 391/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 392/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 393/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 394/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 395/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 396/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 397/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0046\n",
            "Epoch 398/400\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0101\n",
            "Epoch 399/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0093\n",
            "Epoch 400/400\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0114\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f22e5a94f10>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odcm4mv3poXk"
      },
      "source": [
        "Y_pred_test_ann = model.predict(X_test)\n",
        "res = \"\\n\".join(\"{} {}\".format(x, y) for x, y in zip(y_test,Y_pred_test_rf ))\n",
        "#print(res)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwQZ7FfJ9X7h",
        "outputId": "c25628b6-8fc3-4c6f-b052-fdb5ebd5fc14"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, Y_pred_test_ann)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8879676900233026"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8XsOgSuqAUk"
      },
      "source": [
        "# Save the model\n",
        "model.save(\"/content/drive/MyDrive/KIT/solubility_model_88_delney.hdf5\")"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ9G4r8VI6Lr"
      },
      "source": [
        "import pickle\n",
        "#### Save the model :::::\n",
        "#filename = 'finalized_model.rf'\n",
        "pickle.dump(model_xgr, open('/content/drive/MyDrive/KIT/model_96_delnaey_512bit_descriptors.pkl', 'wb'))\n",
        " "
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7EkrwIexv6S",
        "outputId": "5573582a-6309-4cf5-b7b4-fd4251f66412"
      },
      "source": [
        "### Installing RDKIT \n",
        "\n",
        "!mamba install -c conda-forge rdkit"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                  __    __    __    __\n",
            "                 /  \\  /  \\  /  \\  /  \\\n",
            "                /    \\/    \\/    \\/    \\\n",
            "/  //  //  //  /\n",
            "              /  / \\   / \\   / \\   / \\  \\____\n",
            "             /  /   \\_/   \\_/   \\_/   \\    o \\__,\n",
            "            / _/                       \\_____/  `\n",
            "            |/\n",
            "                  \n",
            "          \n",
            "        \n",
            "        \n",
            "                \n",
            "                       \n",
            "\n",
            "        mamba (0.8.0) supported by @QuantStack\n",
            "\n",
            "        GitHub:  https://github.com/mamba-org/mamba\n",
            "        Twitter: https://twitter.com/QuantStack\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Looking for: ['rdkit']\n",
            "\n",
            "conda-forge/linux-64     Using cache\n",
            "conda-forge/noarch       Using cache\n",
            "pkgs/main/noarch         [] (--:--) No change\n",
            "pkgs/main/noarch         [] (00m:00s) No change\n",
            "pkgs/main/noarch         [] (00m:00s) No change\n",
            "pkgs/r/linux-64          [] (--:--) No change\n",
            "pkgs/r/linux-64          [] (00m:00s) No change\n",
            "pkgs/r/linux-64          [] (00m:00s) No change\n",
            "pkgs/r/noarch            [] (--:--) No change\n",
            "pkgs/r/noarch            [] (00m:00s) No change\n",
            "pkgs/r/noarch            [] (00m:00s) No change\n",
            "pkgs/main/linux-64       [] (--:--) No change\n",
            "pkgs/main/linux-64       [] (00m:00s) No change\n",
            "pkgs/main/linux-64       [] (00m:00s) No change\n",
            "Transaction\n",
            "\n",
            "  Prefix: /usr/local\n",
            "\n",
            "  All requested packages already installed\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBH0QZf3x_Qg"
      },
      "source": [
        "from rdkit import Chem\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import rdMolDescriptors"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S1TgaCExUsR"
      },
      "source": [
        "#Turning SMILES into Explicit Bit Vectors (RDKit prefered format)\n",
        "mols = [Chem.rdmolfiles.MolFromSmiles(SMILES_string) for SMILES_string in formated_SMILE_array]\n",
        "\n",
        "test_mols = [Chem.rdmolfiles.MolFromSmiles(test_SMILES_string) for test_SMILES_string in test_formated_SMILE_array]\n",
        "\n",
        "#Convert training molecules into training fingerprints\n",
        "bi = {}\n",
        "fingerprints = [Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(m, radius=2, bitInfo= bi, nBits=256) for m in mols]\n",
        "\n",
        "test_bi = {}\n",
        "test_fingerprints = [rdMolDescriptors.GetMorganFingerprintAsBitVect(test_m, radius=2, bitInfo= bi, nBits=256) for test_m in test_mols]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo2_YOJbxUVG"
      },
      "source": [
        "#Convert training fingerprints into binary, and put all training binaries into arrays\n",
        "import numpy as np \n",
        "\n",
        "fingerprints_array = []\n",
        "for fingerprint in fingerprints:\n",
        "        array = np.zeros((1,), dtype= int)\n",
        "        DataStructs.ConvertToNumpyArray(fingerprint, array)\n",
        "        fingerprints_array.append(array)\n",
        "\n",
        "test_fingerprints_array = []\n",
        "for test_fingerprint in test_fingerprints:\n",
        "        test_array = np.zeros((1,), dtype= int)\n",
        "        DataStructs.ConvertToNumpyArray(test_fingerprint, test_array)\n",
        "        test_fingerprints_array.append(test_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JYMdMvN-H83",
        "outputId": "02f1a1d7-cf87-4ebb-f294-d1ad8bbc343f"
      },
      "source": [
        "### print(len(fingerprints_array))\n",
        "#print(len(solubility_array))\n",
        "#print(type(fingerprints_array))\n",
        "#print(type(solubility_array))\n",
        "#solubility_array = str(solubility_array)\n",
        "#fingerprints_array= str(fingerprints_array)\n",
        "#fingerprints_array=np.array(fingerprints_array)\n",
        "#solubility_array=np.array(solubility_array)###\n",
        "print(fingerprints_array[3])\n",
        "print(solubility_array[999])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            "-3.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMUClpE2Ledf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebsve9QQy3BT",
        "outputId": "e0db59bc-8aa2-4e17-c5ea-57809063cff0"
      },
      "source": [
        "print(fingerprints_array.shape)\n",
        "print(solubility_array.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1144, 256)\n",
            "(1144,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOgS4lBrMJGH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAzMtaKIMO7C"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(fingerprints_array, solubility_array, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wpGK2pEAXBn",
        "outputId": "d8c154ef-206d-42c8-d0ce-a9bdd2f5142c"
      },
      "source": [
        "\"\"\"NEURAL NETWORK\"\"\"\n",
        "#The neural network model\n",
        "model = Sequential([\n",
        "    Dense(256, input_shape=(256,), activation= \"relu\"),\n",
        "    Dense(128, activation= \"tanh\"),\n",
        "    Dense(64, activation= \"tanh\"),\n",
        "    Dense(34, activation= \"tanh\"),\n",
        "    Dense(16, activation= \"tanh\"),\n",
        "    BatchNormalization(axis=1),\n",
        "    Dense(1, activation= \"tanh\")\n",
        "])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_28 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 34)                2210      \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 16)                560       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 109,795\n",
            "Trainable params: 109,763\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13yHJ0v9DMRR"
      },
      "source": [
        "#Compiling the model\n",
        "#opt = .Adam(learning_rate=0.0005)\n",
        "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), metrics=['mae'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9WlIM03DVZl",
        "outputId": "a3a99fcd-7e62-46be-c5e8-dda9ea9985b2"
      },
      "source": [
        "model.fit(X_train, y_train, validation_split=0.2, batch_size=5, epochs= 200, shuffle=True, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4050 - mae: 2.2340 - val_loss: 9.2102 - val_mae: 2.4729\n",
            "Epoch 2/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4063 - mae: 2.2340 - val_loss: 9.2257 - val_mae: 2.4844\n",
            "Epoch 3/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4052 - mae: 2.2331 - val_loss: 9.1799 - val_mae: 2.4532\n",
            "Epoch 4/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4053 - mae: 2.2324 - val_loss: 9.1706 - val_mae: 2.4491\n",
            "Epoch 5/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3999 - mae: 2.2281 - val_loss: 9.1561 - val_mae: 2.4384\n",
            "Epoch 6/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3992 - mae: 2.2285 - val_loss: 9.1931 - val_mae: 2.4605\n",
            "Epoch 7/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4052 - mae: 2.2317 - val_loss: 9.2026 - val_mae: 2.4658\n",
            "Epoch 8/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4020 - mae: 2.2314 - val_loss: 9.1966 - val_mae: 2.4632\n",
            "Epoch 9/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4013 - mae: 2.2307 - val_loss: 9.1882 - val_mae: 2.4551\n",
            "Epoch 10/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3995 - mae: 2.2295 - val_loss: 9.1827 - val_mae: 2.4560\n",
            "Epoch 11/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4006 - mae: 2.2289 - val_loss: 9.1972 - val_mae: 2.4629\n",
            "Epoch 12/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4024 - mae: 2.2304 - val_loss: 9.1810 - val_mae: 2.4569\n",
            "Epoch 13/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4031 - mae: 2.2291 - val_loss: 9.2078 - val_mae: 2.4597\n",
            "Epoch 14/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4047 - mae: 2.2335 - val_loss: 9.1784 - val_mae: 2.4548\n",
            "Epoch 15/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4066 - mae: 2.2351 - val_loss: 9.1821 - val_mae: 2.4555\n",
            "Epoch 16/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4045 - mae: 2.2320 - val_loss: 9.1884 - val_mae: 2.4586\n",
            "Epoch 17/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4016 - mae: 2.2296 - val_loss: 9.2738 - val_mae: 2.4961\n",
            "Epoch 18/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4120 - mae: 2.2349 - val_loss: 9.2173 - val_mae: 2.4733\n",
            "Epoch 19/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4145 - mae: 2.2410 - val_loss: 9.2041 - val_mae: 2.4675\n",
            "Epoch 20/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3971 - mae: 2.2249 - val_loss: 9.2225 - val_mae: 2.4761\n",
            "Epoch 21/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3955 - mae: 2.2251 - val_loss: 9.2438 - val_mae: 2.4844\n",
            "Epoch 22/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4091 - mae: 2.2351 - val_loss: 9.2120 - val_mae: 2.4682\n",
            "Epoch 23/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4016 - mae: 2.2303 - val_loss: 9.1944 - val_mae: 2.4648\n",
            "Epoch 24/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4068 - mae: 2.2349 - val_loss: 9.2054 - val_mae: 2.4692\n",
            "Epoch 25/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4160 - mae: 2.2326 - val_loss: 9.3541 - val_mae: 2.5321\n",
            "Epoch 26/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4084 - mae: 2.2349 - val_loss: 9.2058 - val_mae: 2.4691\n",
            "Epoch 27/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4012 - mae: 2.2279 - val_loss: 9.1687 - val_mae: 2.4477\n",
            "Epoch 28/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3999 - mae: 2.2262 - val_loss: 9.2222 - val_mae: 2.4783\n",
            "Epoch 29/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4006 - mae: 2.2278 - val_loss: 9.2726 - val_mae: 2.5010\n",
            "Epoch 30/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3991 - mae: 2.2274 - val_loss: 9.2623 - val_mae: 2.4987\n",
            "Epoch 31/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4126 - mae: 2.2339 - val_loss: 9.2359 - val_mae: 2.4833\n",
            "Epoch 32/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3982 - mae: 2.2264 - val_loss: 9.2180 - val_mae: 2.4736\n",
            "Epoch 33/200\n",
            "147/147 [==============================] - 1s 3ms/step - loss: 8.4045 - mae: 2.2314 - val_loss: 9.2904 - val_mae: 2.5075\n",
            "Epoch 34/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4000 - mae: 2.2272 - val_loss: 9.2166 - val_mae: 2.4723\n",
            "Epoch 35/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3967 - mae: 2.2235 - val_loss: 9.2732 - val_mae: 2.4970\n",
            "Epoch 36/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3977 - mae: 2.2249 - val_loss: 9.2988 - val_mae: 2.5076\n",
            "Epoch 37/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4022 - mae: 2.2315 - val_loss: 9.2176 - val_mae: 2.4754\n",
            "Epoch 38/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4003 - mae: 2.2271 - val_loss: 9.2180 - val_mae: 2.4740\n",
            "Epoch 39/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4036 - mae: 2.2300 - val_loss: 9.1974 - val_mae: 2.4642\n",
            "Epoch 40/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4049 - mae: 2.2325 - val_loss: 9.2361 - val_mae: 2.4830\n",
            "Epoch 41/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4024 - mae: 2.2317 - val_loss: 9.1610 - val_mae: 2.4458\n",
            "Epoch 42/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3992 - mae: 2.2273 - val_loss: 9.2666 - val_mae: 2.4999\n",
            "Epoch 43/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3962 - mae: 2.2224 - val_loss: 9.1890 - val_mae: 2.4612\n",
            "Epoch 44/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3978 - mae: 2.2248 - val_loss: 9.1910 - val_mae: 2.4634\n",
            "Epoch 45/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3951 - mae: 2.2212 - val_loss: 9.2183 - val_mae: 2.4741\n",
            "Epoch 46/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3987 - mae: 2.2257 - val_loss: 9.1941 - val_mae: 2.4632\n",
            "Epoch 47/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4045 - mae: 2.2315 - val_loss: 9.1631 - val_mae: 2.4483\n",
            "Epoch 48/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4017 - mae: 2.2285 - val_loss: 9.1882 - val_mae: 2.4610\n",
            "Epoch 49/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4019 - mae: 2.2281 - val_loss: 9.2013 - val_mae: 2.4607\n",
            "Epoch 50/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3982 - mae: 2.2241 - val_loss: 9.2145 - val_mae: 2.4651\n",
            "Epoch 51/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3991 - mae: 2.2262 - val_loss: 9.2197 - val_mae: 2.4685\n",
            "Epoch 52/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3990 - mae: 2.2246 - val_loss: 9.2161 - val_mae: 2.4644\n",
            "Epoch 53/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4081 - mae: 2.2295 - val_loss: 9.1978 - val_mae: 2.4620\n",
            "Epoch 54/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4055 - mae: 2.2284 - val_loss: 9.2366 - val_mae: 2.4754\n",
            "Epoch 55/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3978 - mae: 2.2234 - val_loss: 9.2739 - val_mae: 2.4969\n",
            "Epoch 56/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4029 - mae: 2.2272 - val_loss: 9.2020 - val_mae: 2.4658\n",
            "Epoch 57/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3954 - mae: 2.2198 - val_loss: 9.2336 - val_mae: 2.4751\n",
            "Epoch 58/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4021 - mae: 2.2282 - val_loss: 9.2272 - val_mae: 2.4741\n",
            "Epoch 59/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4134 - mae: 2.2360 - val_loss: 9.2227 - val_mae: 2.4719\n",
            "Epoch 60/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4032 - mae: 2.2283 - val_loss: 9.1844 - val_mae: 2.4598\n",
            "Epoch 61/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3989 - mae: 2.2232 - val_loss: 9.1849 - val_mae: 2.4606\n",
            "Epoch 62/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4079 - mae: 2.2321 - val_loss: 9.2214 - val_mae: 2.4751\n",
            "Epoch 63/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4026 - mae: 2.2277 - val_loss: 9.2020 - val_mae: 2.4670\n",
            "Epoch 64/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4006 - mae: 2.2253 - val_loss: 9.2049 - val_mae: 2.4678\n",
            "Epoch 65/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4003 - mae: 2.2240 - val_loss: 9.1820 - val_mae: 2.4545\n",
            "Epoch 66/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4072 - mae: 2.2316 - val_loss: 9.1835 - val_mae: 2.4560\n",
            "Epoch 67/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4021 - mae: 2.2282 - val_loss: 9.2084 - val_mae: 2.4639\n",
            "Epoch 68/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4040 - mae: 2.2287 - val_loss: 9.2086 - val_mae: 2.4626\n",
            "Epoch 69/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3998 - mae: 2.2246 - val_loss: 9.1970 - val_mae: 2.4614\n",
            "Epoch 70/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4133 - mae: 2.2321 - val_loss: 9.1732 - val_mae: 2.4491\n",
            "Epoch 71/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4115 - mae: 2.2378 - val_loss: 9.1964 - val_mae: 2.4641\n",
            "Epoch 72/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4022 - mae: 2.2288 - val_loss: 9.2377 - val_mae: 2.4684\n",
            "Epoch 73/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3976 - mae: 2.2247 - val_loss: 9.2460 - val_mae: 2.4722\n",
            "Epoch 74/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4002 - mae: 2.2260 - val_loss: 9.2378 - val_mae: 2.4699\n",
            "Epoch 75/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3991 - mae: 2.2249 - val_loss: 9.2593 - val_mae: 2.4775\n",
            "Epoch 76/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4091 - mae: 2.2281 - val_loss: 9.2535 - val_mae: 2.4782\n",
            "Epoch 77/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3977 - mae: 2.2232 - val_loss: 9.2190 - val_mae: 2.4705\n",
            "Epoch 78/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4020 - mae: 2.2283 - val_loss: 9.2134 - val_mae: 2.4647\n",
            "Epoch 79/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3989 - mae: 2.2231 - val_loss: 9.1812 - val_mae: 2.4555\n",
            "Epoch 80/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3956 - mae: 2.2214 - val_loss: 9.1565 - val_mae: 2.4458\n",
            "Epoch 81/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4032 - mae: 2.2286 - val_loss: 9.1523 - val_mae: 2.4423\n",
            "Epoch 82/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4001 - mae: 2.2259 - val_loss: 9.1755 - val_mae: 2.4523\n",
            "Epoch 83/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3987 - mae: 2.2262 - val_loss: 9.2590 - val_mae: 2.4768\n",
            "Epoch 84/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3962 - mae: 2.2225 - val_loss: 9.2503 - val_mae: 2.4741\n",
            "Epoch 85/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3985 - mae: 2.2255 - val_loss: 9.2529 - val_mae: 2.4747\n",
            "Epoch 86/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3991 - mae: 2.2235 - val_loss: 9.2647 - val_mae: 2.4789\n",
            "Epoch 87/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3964 - mae: 2.2209 - val_loss: 9.2672 - val_mae: 2.4818\n",
            "Epoch 88/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3982 - mae: 2.2255 - val_loss: 9.2891 - val_mae: 2.4924\n",
            "Epoch 89/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4018 - mae: 2.2285 - val_loss: 9.2871 - val_mae: 2.4926\n",
            "Epoch 90/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3953 - mae: 2.2202 - val_loss: 9.3167 - val_mae: 2.5085\n",
            "Epoch 91/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4006 - mae: 2.2289 - val_loss: 9.2095 - val_mae: 2.4696\n",
            "Epoch 92/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3971 - mae: 2.2229 - val_loss: 9.2968 - val_mae: 2.4933\n",
            "Epoch 93/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3927 - mae: 2.2180 - val_loss: 9.2256 - val_mae: 2.4684\n",
            "Epoch 94/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4016 - mae: 2.2267 - val_loss: 9.2412 - val_mae: 2.4728\n",
            "Epoch 95/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3994 - mae: 2.2255 - val_loss: 9.2646 - val_mae: 2.4804\n",
            "Epoch 96/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3953 - mae: 2.2209 - val_loss: 9.2559 - val_mae: 2.4761\n",
            "Epoch 97/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3933 - mae: 2.2183 - val_loss: 9.2470 - val_mae: 2.4742\n",
            "Epoch 98/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3952 - mae: 2.2183 - val_loss: 9.2338 - val_mae: 2.4664\n",
            "Epoch 99/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3947 - mae: 2.2192 - val_loss: 9.2438 - val_mae: 2.4704\n",
            "Epoch 100/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4047 - mae: 2.2262 - val_loss: 9.2457 - val_mae: 2.4690\n",
            "Epoch 101/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4145 - mae: 2.2331 - val_loss: 9.2392 - val_mae: 2.4713\n",
            "Epoch 102/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3965 - mae: 2.2232 - val_loss: 9.2435 - val_mae: 2.4684\n",
            "Epoch 103/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4001 - mae: 2.2279 - val_loss: 9.2457 - val_mae: 2.4711\n",
            "Epoch 104/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3942 - mae: 2.2183 - val_loss: 9.2457 - val_mae: 2.4724\n",
            "Epoch 105/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3975 - mae: 2.2245 - val_loss: 9.2449 - val_mae: 2.4711\n",
            "Epoch 106/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3947 - mae: 2.2211 - val_loss: 9.2461 - val_mae: 2.4738\n",
            "Epoch 107/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3999 - mae: 2.2232 - val_loss: 9.2455 - val_mae: 2.4710\n",
            "Epoch 108/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4054 - mae: 2.2311 - val_loss: 9.2460 - val_mae: 2.4722\n",
            "Epoch 109/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3966 - mae: 2.2225 - val_loss: 9.2510 - val_mae: 2.4763\n",
            "Epoch 110/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3942 - mae: 2.2205 - val_loss: 9.2035 - val_mae: 2.4611\n",
            "Epoch 111/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3995 - mae: 2.2232 - val_loss: 9.2075 - val_mae: 2.4661\n",
            "Epoch 112/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4035 - mae: 2.2280 - val_loss: 9.1911 - val_mae: 2.4602\n",
            "Epoch 113/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4006 - mae: 2.2262 - val_loss: 9.1910 - val_mae: 2.4606\n",
            "Epoch 114/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4011 - mae: 2.2250 - val_loss: 9.1924 - val_mae: 2.4608\n",
            "Epoch 115/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4011 - mae: 2.2240 - val_loss: 9.1911 - val_mae: 2.4599\n",
            "Epoch 116/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4030 - mae: 2.2286 - val_loss: 9.1909 - val_mae: 2.4606\n",
            "Epoch 117/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4041 - mae: 2.2282 - val_loss: 9.1882 - val_mae: 2.4558\n",
            "Epoch 118/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4011 - mae: 2.2248 - val_loss: 9.1935 - val_mae: 2.4579\n",
            "Epoch 119/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3972 - mae: 2.2214 - val_loss: 9.1911 - val_mae: 2.4601\n",
            "Epoch 120/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4095 - mae: 2.2329 - val_loss: 9.1969 - val_mae: 2.4606\n",
            "Epoch 121/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4068 - mae: 2.2272 - val_loss: 9.1912 - val_mae: 2.4601\n",
            "Epoch 122/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4167 - mae: 2.2388 - val_loss: 9.1910 - val_mae: 2.4599\n",
            "Epoch 123/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4103 - mae: 2.2316 - val_loss: 9.1906 - val_mae: 2.4594\n",
            "Epoch 124/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4068 - mae: 2.2291 - val_loss: 9.1912 - val_mae: 2.4601\n",
            "Epoch 125/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4105 - mae: 2.2343 - val_loss: 9.2312 - val_mae: 2.4746\n",
            "Epoch 126/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4007 - mae: 2.2239 - val_loss: 9.1903 - val_mae: 2.4593\n",
            "Epoch 127/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3981 - mae: 2.2233 - val_loss: 9.1889 - val_mae: 2.4579\n",
            "Epoch 128/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3973 - mae: 2.2233 - val_loss: 9.1942 - val_mae: 2.4629\n",
            "Epoch 129/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4147 - mae: 2.2348 - val_loss: 9.1906 - val_mae: 2.4595\n",
            "Epoch 130/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4094 - mae: 2.2312 - val_loss: 9.1908 - val_mae: 2.4596\n",
            "Epoch 131/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4060 - mae: 2.2296 - val_loss: 9.1987 - val_mae: 2.4632\n",
            "Epoch 132/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4115 - mae: 2.2336 - val_loss: 9.1988 - val_mae: 2.4626\n",
            "Epoch 133/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4088 - mae: 2.2292 - val_loss: 9.1949 - val_mae: 2.4568\n",
            "Epoch 134/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4091 - mae: 2.2306 - val_loss: 9.1991 - val_mae: 2.4637\n",
            "Epoch 135/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4042 - mae: 2.2268 - val_loss: 9.1978 - val_mae: 2.4623\n",
            "Epoch 136/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4015 - mae: 2.2256 - val_loss: 9.1989 - val_mae: 2.4637\n",
            "Epoch 137/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4045 - mae: 2.2259 - val_loss: 9.1967 - val_mae: 2.4608\n",
            "Epoch 138/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3959 - mae: 2.2215 - val_loss: 9.1981 - val_mae: 2.4625\n",
            "Epoch 139/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4041 - mae: 2.2265 - val_loss: 9.2046 - val_mae: 2.4667\n",
            "Epoch 140/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3981 - mae: 2.2225 - val_loss: 9.2192 - val_mae: 2.4726\n",
            "Epoch 141/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3950 - mae: 2.2205 - val_loss: 9.1987 - val_mae: 2.4633\n",
            "Epoch 142/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3969 - mae: 2.2227 - val_loss: 9.2137 - val_mae: 2.4731\n",
            "Epoch 143/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3964 - mae: 2.2218 - val_loss: 9.1977 - val_mae: 2.4619\n",
            "Epoch 144/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3970 - mae: 2.2230 - val_loss: 9.1977 - val_mae: 2.4614\n",
            "Epoch 145/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3981 - mae: 2.2221 - val_loss: 9.1982 - val_mae: 2.4625\n",
            "Epoch 146/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3991 - mae: 2.2238 - val_loss: 9.1972 - val_mae: 2.4615\n",
            "Epoch 147/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3987 - mae: 2.2225 - val_loss: 9.1982 - val_mae: 2.4628\n",
            "Epoch 148/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3955 - mae: 2.2209 - val_loss: 9.2155 - val_mae: 2.4725\n",
            "Epoch 149/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3926 - mae: 2.2175 - val_loss: 9.2644 - val_mae: 2.4895\n",
            "Epoch 150/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3924 - mae: 2.2183 - val_loss: 9.2429 - val_mae: 2.4800\n",
            "Epoch 151/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3920 - mae: 2.2150 - val_loss: 9.2011 - val_mae: 2.4660\n",
            "Epoch 152/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3987 - mae: 2.2237 - val_loss: 9.1996 - val_mae: 2.4641\n",
            "Epoch 153/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3952 - mae: 2.2212 - val_loss: 9.2129 - val_mae: 2.4711\n",
            "Epoch 154/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4012 - mae: 2.2251 - val_loss: 9.2149 - val_mae: 2.4721\n",
            "Epoch 155/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4182 - mae: 2.2385 - val_loss: 9.2182 - val_mae: 2.4743\n",
            "Epoch 156/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3931 - mae: 2.2193 - val_loss: 9.2898 - val_mae: 2.5034\n",
            "Epoch 157/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3944 - mae: 2.2214 - val_loss: 9.3900 - val_mae: 2.5394\n",
            "Epoch 158/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3938 - mae: 2.2192 - val_loss: 9.2932 - val_mae: 2.5044\n",
            "Epoch 159/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3973 - mae: 2.2219 - val_loss: 9.2935 - val_mae: 2.5045\n",
            "Epoch 160/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4000 - mae: 2.2277 - val_loss: 9.4866 - val_mae: 2.5344\n",
            "Epoch 161/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4026 - mae: 2.2298 - val_loss: 9.2430 - val_mae: 2.4872\n",
            "Epoch 162/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3964 - mae: 2.2218 - val_loss: 9.2981 - val_mae: 2.5063\n",
            "Epoch 163/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3976 - mae: 2.2203 - val_loss: 9.2956 - val_mae: 2.5057\n",
            "Epoch 164/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4028 - mae: 2.2239 - val_loss: 9.2978 - val_mae: 2.5084\n",
            "Epoch 165/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3917 - mae: 2.2181 - val_loss: 9.2908 - val_mae: 2.5031\n",
            "Epoch 166/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3992 - mae: 2.2239 - val_loss: 9.2533 - val_mae: 2.4906\n",
            "Epoch 167/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3955 - mae: 2.2214 - val_loss: 9.2566 - val_mae: 2.4930\n",
            "Epoch 168/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.4013 - mae: 2.2246 - val_loss: 9.2605 - val_mae: 2.4946\n",
            "Epoch 169/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3965 - mae: 2.2208 - val_loss: 9.2409 - val_mae: 2.4858\n",
            "Epoch 170/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3949 - mae: 2.2200 - val_loss: 9.2362 - val_mae: 2.4839\n",
            "Epoch 171/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3921 - mae: 2.2188 - val_loss: 9.2134 - val_mae: 2.4714\n",
            "Epoch 172/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3931 - mae: 2.2181 - val_loss: 9.2173 - val_mae: 2.4723\n",
            "Epoch 173/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3968 - mae: 2.2229 - val_loss: 9.2173 - val_mae: 2.4732\n",
            "Epoch 174/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3966 - mae: 2.2233 - val_loss: 9.2172 - val_mae: 2.4755\n",
            "Epoch 175/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3955 - mae: 2.2218 - val_loss: 9.2713 - val_mae: 2.5005\n",
            "Epoch 176/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3935 - mae: 2.2197 - val_loss: 9.2392 - val_mae: 2.4833\n",
            "Epoch 177/200\n",
            "147/147 [==============================] - 1s 6ms/step - loss: 8.3896 - mae: 2.2151 - val_loss: 9.2652 - val_mae: 2.4986\n",
            "Epoch 178/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3983 - mae: 2.2234 - val_loss: 9.2391 - val_mae: 2.4839\n",
            "Epoch 179/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4074 - mae: 2.2258 - val_loss: 9.2421 - val_mae: 2.4873\n",
            "Epoch 180/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3952 - mae: 2.2189 - val_loss: 9.2612 - val_mae: 2.4953\n",
            "Epoch 181/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3917 - mae: 2.2157 - val_loss: 9.2392 - val_mae: 2.4839\n",
            "Epoch 182/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3931 - mae: 2.2180 - val_loss: 9.3332 - val_mae: 2.5139\n",
            "Epoch 183/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3883 - mae: 2.2116 - val_loss: 9.2600 - val_mae: 2.4950\n",
            "Epoch 184/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3978 - mae: 2.2219 - val_loss: 9.2187 - val_mae: 2.4745\n",
            "Epoch 185/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3922 - mae: 2.2171 - val_loss: 9.2479 - val_mae: 2.4899\n",
            "Epoch 186/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3975 - mae: 2.2212 - val_loss: 9.2392 - val_mae: 2.4839\n",
            "Epoch 187/200\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 8.3962 - mae: 2.2217 - val_loss: 9.2181 - val_mae: 2.4746\n",
            "Epoch 188/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4053 - mae: 2.2264 - val_loss: 9.2992 - val_mae: 2.5021\n",
            "Epoch 189/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4019 - mae: 2.2275 - val_loss: 9.3172 - val_mae: 2.5061\n",
            "Epoch 190/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4055 - mae: 2.2287 - val_loss: 9.3173 - val_mae: 2.5061\n",
            "Epoch 191/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3954 - mae: 2.2181 - val_loss: 9.3354 - val_mae: 2.5152\n",
            "Epoch 192/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4004 - mae: 2.2239 - val_loss: 9.3285 - val_mae: 2.5130\n",
            "Epoch 193/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3895 - mae: 2.2150 - val_loss: 9.3354 - val_mae: 2.5152\n",
            "Epoch 194/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4000 - mae: 2.2231 - val_loss: 9.3354 - val_mae: 2.5151\n",
            "Epoch 195/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3928 - mae: 2.2173 - val_loss: 9.3324 - val_mae: 2.5146\n",
            "Epoch 196/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3954 - mae: 2.2207 - val_loss: 9.2922 - val_mae: 2.5041\n",
            "Epoch 197/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3880 - mae: 2.2105 - val_loss: 9.2395 - val_mae: 2.4843\n",
            "Epoch 198/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3905 - mae: 2.2141 - val_loss: 9.2602 - val_mae: 2.4917\n",
            "Epoch 199/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.4067 - mae: 2.2293 - val_loss: 9.2633 - val_mae: 2.4923\n",
            "Epoch 200/200\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 8.3966 - mae: 2.2223 - val_loss: 9.2748 - val_mae: 2.4951\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4b309e5550>"
            ]
          },
          "metadata": {},
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z0O170Ry1rf"
      },
      "source": [
        "smiles_input = Input(shape=(max_smiles_chars, charset_length), name=\"SMILES-Input\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNhIQVOP2xAb"
      },
      "source": [
        "print(fingerprints_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8JD4yrOyGBi"
      },
      "source": [
        "# general imports\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# keras imports\n",
        "from keras.layers import (Input, Dense, Conv1D, MaxPool1D, Dropout, GRU, LSTM, \n",
        "                          TimeDistributed, Add, Flatten, RepeatVector, Lambda, Concatenate)\n",
        "from keras.models import Model, load_model\n",
        "from keras.metrics import binary_crossentropy\n",
        "from keras import initializers\n",
        "import keras.backend as K\n",
        "\n",
        "# Visualization\n",
        "#from keras_sequential_ascii import keras2ascii\n",
        "\n",
        "# utils functions\n",
        "#from utils import *\n",
        "\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "# Remove warnings from output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQb70h1UyWaV"
      },
      "source": [
        "# Set parameters for convolutional layers \n",
        "num_conv_filters = 16\n",
        "kernel_size = 3\n",
        "\n",
        "init_weights = initializers.glorot_normal(seed=0)\n",
        "\n",
        "# Define the convolutional layers\n",
        "# Multiple convolutions in a row is a common architecture (but there are many \"right\" choices here)\n",
        "conv_1_func = Conv1D(\n",
        "    filters=num_conv_filters, # What is the \"depth\" of the convolution? How many times do you look at the same spot?\n",
        "    kernel_size=kernel_size, # How \"wide\" of a spot does each filter look at?\n",
        "    name=\"Convolution-1\",\n",
        "    activation=\"relu\", # This is a common activation function: Rectified Linear Unit (ReLU)\n",
        "    kernel_initializer=init_weights #This defines the initial values for the weights\n",
        ")\n",
        "conv_2_func = Conv1D(\n",
        "    filters=num_conv_filters, \n",
        "    kernel_size=kernel_size, \n",
        "    name=\"Convolution-2\",\n",
        "    activation=\"relu\",\n",
        "    kernel_initializer=init_weights\n",
        ")\n",
        "conv_3_func = Conv1D(\n",
        "    filters=num_conv_filters, \n",
        "    kernel_size=kernel_size, \n",
        "    name=\"Convolution-3\",\n",
        "    activation=\"relu\",\n",
        "    kernel_initializer=init_weights\n",
        ")\n",
        "conv_4_func = Conv1D(\n",
        "    filters=num_conv_filters, \n",
        "    kernel_size=kernel_size,\n",
        "    name=\"Convolution-4\",\n",
        "    activation=\"relu\",\n",
        "    kernel_initializer=init_weights\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1qPo_4GycpN"
      },
      "source": [
        "# Define layer to flatten convolutions\n",
        "flatten_func = Flatten(name=\"Flattened-Convolutions\")\n",
        "\n",
        "# Define the activation function layer\n",
        "hidden_size = 32\n",
        "dense_1_func = Dense(hidden_size, activation=\"relu\", name=\"Fully-Connected\", kernel_initializer=init_weights)\n",
        "\n",
        "# Define output layer -- it's only one dimension since it is regression\n",
        "output_size = 1\n",
        "output_solubility_func = Dense(output_size, activation=\"linear\", name=\"Log-Solubility\", kernel_initializer=init_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0--m66h0zqyG"
      },
      "source": [
        "smiles_input = Input(shape=(1198, 256,1), name=\"SMILES-Input\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "gknq6d_XyhdV",
        "outputId": "ef0ebc4c-b325-4af7-8ae1-e17cf53fe388"
      },
      "source": [
        "# connect the CNN graph together\n",
        "conv_1_fwd = conv_1_func(smiles_input)\n",
        "conv_2_fwd = conv_2_func(conv_1_fwd)\n",
        "conv_3_fwd = conv_3_func(conv_2_fwd)\n",
        "conv_4_fwd = conv_4_func(conv_3_fwd)\n",
        "flattened_convs = flatten_func(conv_4_fwd)\n",
        "dense_1_fwd = dense_1_func(flattened_convs)\n",
        "output_solubility_fwd = output_solubility_func(flattened_convs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1879\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1881\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Depth of input (1) is not a multiple of input depth of filter (256) for '{{node Convolution-1/conv1d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Convolution-1/conv1d/Reshape, Convolution-1/conv1d/ExpandDims_1)' with input shapes: [?,1,256,1], [1,3,256,16].",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-247-6cad4d2cb19c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# connect the CNN graph together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconv_1_fwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_1_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mconv_2_fwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_2_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_1_fwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconv_3_fwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_3_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_2_fwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconv_4_fwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_4_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_3_fwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 977\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1113\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1115\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    886\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1136\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(value, filters, stride, padding, use_cudnn_on_gpu, data_format, name, input, dilations)\u001b[0m\n\u001b[1;32m   2023\u001b[0m           ),\n\u001b[1;32m   2024\u001b[0m           \u001b[0minner_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2025\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   2026\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspatial_start_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msqueeze_batch_dims\u001b[0;34m(inp, op, inner_rank, name)\u001b[0m\n\u001b[1;32m    430\u001b[0m           inp, array_ops.concat(([-1], inner_shape), axis=-1))\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     \u001b[0mout_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0mout_inner_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_reshaped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0minner_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    971\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                   \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m                   data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[1;32m    974\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    748\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    749\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    599\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    600\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3567\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3568\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3569\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3570\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3571\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2041\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0;32m-> 2042\u001b[0;31m                                 control_input_ops, op_def)\n\u001b[0m\u001b[1;32m   2043\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1881\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Depth of input (1) is not a multiple of input depth of filter (256) for '{{node Convolution-1/conv1d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Convolution-1/conv1d/Reshape, Convolution-1/conv1d/ExpandDims_1)' with input shapes: [?,1,256,1], [1,3,256,16]."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34dfHzygUiwR"
      },
      "source": [
        "# create model\n",
        "solubility_model = Model(\n",
        "            inputs=[smiles_input],\n",
        "            outputs=[output_solubility_fwd]\n",
        ")\n",
        "\n",
        "# compile model\n",
        "solubility_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"mse\",\n",
        "    metrics=[\"mae\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77bF6pVm0umJ",
        "outputId": "2cad4f95-0392-42fb-bd36-a571212356ad"
      },
      "source": [
        "from numpy import zeros, newaxis\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "#X_train = X_train[:, :, newaxis]\n",
        "#X_test = X_test[:, :, newaxis]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(915, 256, 1)\n",
            "(915,)\n",
            "(229, 256, 1)\n",
            "(229,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "id": "sb9Rj47wz810",
        "outputId": "829d4196-23b5-4b5f-f251-e3dbb8c95c5e"
      },
      "source": [
        "history = solubility_model.fit(\n",
        "    X_train, # Inputs\n",
        "    y_train, # Outputs\n",
        "    epochs=115, # How many times to pass over the data\n",
        "    batch_size=64, # How many data rows to compute at once\n",
        "    verbose=1,\n",
        "    validation_data=(X_test, y_test), # You would usually use more splits of the data if you plan to tune hyperparams\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/115\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-206-5ed1dd5e4619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# How many data rows to compute at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# You would usually use more splits of the data if you plan to tune hyperparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m-> 3038\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3458\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3459\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3460\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3381\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3382\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3383\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:787 train_step\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py:269 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 1198, 256), found shape=(None, 256, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJq6vAHi0tgj",
        "outputId": "8e125dd0-0690-4c58-836f-166b80f70432"
      },
      "source": [
        "\"\"\"NEURAL NETWORK\"\"\"\n",
        "#The neural network model\n",
        "model = Sequential([\n",
        "    Dense(256, input_shape=(256,), activation= \"relu\"),\n",
        "    Dense(128, activation= \"tanh\"),\n",
        "    Dense(64, activation= \"tanh\"),\n",
        "    Dense(34, activation= \"tanh\"),\n",
        "    Dense(16, activation= \"tanh\"),\n",
        "    BatchNormalization(axis=1),\n",
        "    Dense(1, activation= \"tanh\")\n",
        "])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_18 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 34)                2210      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 16)                560       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 109,795\n",
            "Trainable params: 109,763\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNpO5Ycg1KTX"
      },
      "source": [
        "#Compiling the model\n",
        "#opt = .Adam(learning_rate=0.0005)\n",
        "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), metrics=['mae'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMFZOpbQ76qi",
        "outputId": "6755b392-0c65-4642-e02f-df51928b3f49"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs=200,\n",
        "                    batch_size=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2140 - mae: 2.2480 - val_loss: 9.7523 - val_mae: 2.4328\n",
            "Epoch 2/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2146 - mae: 2.2474 - val_loss: 9.7523 - val_mae: 2.4328\n",
            "Epoch 3/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2146 - mae: 2.2484 - val_loss: 9.7522 - val_mae: 2.4328\n",
            "Epoch 4/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2143 - mae: 2.2474 - val_loss: 9.7516 - val_mae: 2.4326\n",
            "Epoch 5/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2147 - mae: 2.2479 - val_loss: 9.7441 - val_mae: 2.4279\n",
            "Epoch 6/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2132 - mae: 2.2467 - val_loss: 9.7523 - val_mae: 2.4328\n",
            "Epoch 7/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2152 - mae: 2.2488 - val_loss: 9.7523 - val_mae: 2.4328\n",
            "Epoch 8/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2145 - mae: 2.2481 - val_loss: 9.7524 - val_mae: 2.4328\n",
            "Epoch 9/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2148 - mae: 2.2483 - val_loss: 9.7524 - val_mae: 2.4328\n",
            "Epoch 10/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2154 - mae: 2.2488 - val_loss: 9.7521 - val_mae: 2.4327\n",
            "Epoch 11/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2103 - mae: 2.2434 - val_loss: 9.7515 - val_mae: 2.4324\n",
            "Epoch 12/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2154 - mae: 2.2484 - val_loss: 9.7498 - val_mae: 2.4318\n",
            "Epoch 13/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2128 - mae: 2.2470 - val_loss: 9.7447 - val_mae: 2.4287\n",
            "Epoch 14/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2119 - mae: 2.2456 - val_loss: 9.7451 - val_mae: 2.4289\n",
            "Epoch 15/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2137 - mae: 2.2469 - val_loss: 9.7447 - val_mae: 2.4287\n",
            "Epoch 16/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2213 - mae: 2.2521 - val_loss: 9.7518 - val_mae: 2.4326\n",
            "Epoch 17/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2150 - mae: 2.2474 - val_loss: 9.7524 - val_mae: 2.4328\n",
            "Epoch 18/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2187 - mae: 2.2495 - val_loss: 9.7524 - val_mae: 2.4328\n",
            "Epoch 19/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2135 - mae: 2.2466 - val_loss: 9.7583 - val_mae: 2.4336\n",
            "Epoch 20/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2135 - mae: 2.2462 - val_loss: 9.7525 - val_mae: 2.4328\n",
            "Epoch 21/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2144 - mae: 2.2480 - val_loss: 9.7851 - val_mae: 2.4416\n",
            "Epoch 22/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2128 - mae: 2.2466 - val_loss: 9.7693 - val_mae: 2.4384\n",
            "Epoch 23/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2144 - mae: 2.2471 - val_loss: 9.7850 - val_mae: 2.4415\n",
            "Epoch 24/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2144 - mae: 2.2467 - val_loss: 9.7851 - val_mae: 2.4416\n",
            "Epoch 25/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2110 - mae: 2.2456 - val_loss: 9.7456 - val_mae: 2.4294\n",
            "Epoch 26/200\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 8.2121 - mae: 2.2467 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 27/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2098 - mae: 2.2444 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 28/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2100 - mae: 2.2426 - val_loss: 9.7454 - val_mae: 2.4293\n",
            "Epoch 29/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2115 - mae: 2.2452 - val_loss: 9.7455 - val_mae: 2.4294\n",
            "Epoch 30/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2076 - mae: 2.2436 - val_loss: 9.7454 - val_mae: 2.4293\n",
            "Epoch 31/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2052 - mae: 2.2403 - val_loss: 9.7454 - val_mae: 2.4293\n",
            "Epoch 32/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2098 - mae: 2.2418 - val_loss: 9.7454 - val_mae: 2.4293\n",
            "Epoch 33/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2134 - mae: 2.2455 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 34/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2164 - mae: 2.2472 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 35/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2024 - mae: 2.2358 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 36/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2138 - mae: 2.2454 - val_loss: 9.7455 - val_mae: 2.4293\n",
            "Epoch 37/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2061 - mae: 2.2404 - val_loss: 9.7455 - val_mae: 2.4293\n",
            "Epoch 38/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2101 - mae: 2.2441 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 39/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2049 - mae: 2.2400 - val_loss: 9.7454 - val_mae: 2.4293\n",
            "Epoch 40/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2065 - mae: 2.2411 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 41/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2048 - mae: 2.2401 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 42/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2047 - mae: 2.2399 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 43/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2068 - mae: 2.2415 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 44/200\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 8.2068 - mae: 2.2423 - val_loss: 9.7456 - val_mae: 2.4302\n",
            "Epoch 45/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2076 - mae: 2.2426 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 46/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2071 - mae: 2.2417 - val_loss: 9.7450 - val_mae: 2.4292\n",
            "Epoch 47/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2040 - mae: 2.2387 - val_loss: 9.7451 - val_mae: 2.4292\n",
            "Epoch 48/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2040 - mae: 2.2373 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 49/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2059 - mae: 2.2404 - val_loss: 9.7452 - val_mae: 2.4293\n",
            "Epoch 50/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2055 - mae: 2.2405 - val_loss: 9.7449 - val_mae: 2.4291\n",
            "Epoch 51/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2021 - mae: 2.2368 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 52/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2014 - mae: 2.2355 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 53/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2014 - mae: 2.2381 - val_loss: 9.7454 - val_mae: 2.4293\n",
            "Epoch 54/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1996 - mae: 2.2357 - val_loss: 9.7454 - val_mae: 2.4294\n",
            "Epoch 55/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2017 - mae: 2.2376 - val_loss: 9.7466 - val_mae: 2.4299\n",
            "Epoch 56/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2077 - mae: 2.2403 - val_loss: 9.7501 - val_mae: 2.4314\n",
            "Epoch 57/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2002 - mae: 2.2381 - val_loss: 9.7466 - val_mae: 2.4300\n",
            "Epoch 58/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2097 - mae: 2.2443 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 59/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2063 - mae: 2.2420 - val_loss: 9.7454 - val_mae: 2.4295\n",
            "Epoch 60/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2067 - mae: 2.2415 - val_loss: 9.7385 - val_mae: 2.4231\n",
            "Epoch 61/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1990 - mae: 2.2341 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 62/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1964 - mae: 2.2336 - val_loss: 9.7476 - val_mae: 2.4296\n",
            "Epoch 63/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.2042 - mae: 2.2399 - val_loss: 9.7453 - val_mae: 2.4293\n",
            "Epoch 64/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.1992 - mae: 2.2354 - val_loss: 9.7452 - val_mae: 2.4293\n",
            "Epoch 65/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.2028 - mae: 2.2377 - val_loss: 9.7388 - val_mae: 2.4265\n",
            "Epoch 66/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2024 - mae: 2.2385 - val_loss: 9.7364 - val_mae: 2.4241\n",
            "Epoch 67/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1994 - mae: 2.2359 - val_loss: 9.7360 - val_mae: 2.4229\n",
            "Epoch 68/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.2008 - mae: 2.2374 - val_loss: 9.7391 - val_mae: 2.4267\n",
            "Epoch 69/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.1986 - mae: 2.2348 - val_loss: 9.7386 - val_mae: 2.4263\n",
            "Epoch 70/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1985 - mae: 2.2361 - val_loss: 9.7811 - val_mae: 2.4349\n",
            "Epoch 71/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2047 - mae: 2.2395 - val_loss: 9.7873 - val_mae: 2.4377\n",
            "Epoch 72/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.2031 - mae: 2.2381 - val_loss: 9.7796 - val_mae: 2.4336\n",
            "Epoch 73/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.2014 - mae: 2.2373 - val_loss: 9.7800 - val_mae: 2.4337\n",
            "Epoch 74/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.2042 - mae: 2.2393 - val_loss: 9.7797 - val_mae: 2.4337\n",
            "Epoch 75/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.2003 - mae: 2.2367 - val_loss: 9.7793 - val_mae: 2.4336\n",
            "Epoch 76/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1958 - mae: 2.2320 - val_loss: 9.7773 - val_mae: 2.4331\n",
            "Epoch 77/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.2021 - mae: 2.2392 - val_loss: 9.7786 - val_mae: 2.4334\n",
            "Epoch 78/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.1961 - mae: 2.2339 - val_loss: 9.7764 - val_mae: 2.4331\n",
            "Epoch 79/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1967 - mae: 2.2336 - val_loss: 9.7416 - val_mae: 2.4252\n",
            "Epoch 80/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.2015 - mae: 2.2385 - val_loss: 9.7418 - val_mae: 2.4280\n",
            "Epoch 81/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1920 - mae: 2.2289 - val_loss: 9.7369 - val_mae: 2.4245\n",
            "Epoch 82/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1925 - mae: 2.2299 - val_loss: 9.7413 - val_mae: 2.4275\n",
            "Epoch 83/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1978 - mae: 2.2344 - val_loss: 9.7428 - val_mae: 2.4277\n",
            "Epoch 84/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1976 - mae: 2.2343 - val_loss: 9.7371 - val_mae: 2.4248\n",
            "Epoch 85/200\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 8.1878 - mae: 2.2289 - val_loss: 9.7454 - val_mae: 2.4293\n",
            "Epoch 86/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.1930 - mae: 2.2310 - val_loss: 9.7619 - val_mae: 2.4376\n",
            "Epoch 87/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1920 - mae: 2.2314 - val_loss: 9.7619 - val_mae: 2.4376\n",
            "Epoch 88/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.1897 - mae: 2.2291 - val_loss: 9.7619 - val_mae: 2.4376\n",
            "Epoch 89/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1913 - mae: 2.2309 - val_loss: 9.7618 - val_mae: 2.4376\n",
            "Epoch 90/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1923 - mae: 2.2330 - val_loss: 9.7619 - val_mae: 2.4376\n",
            "Epoch 91/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1879 - mae: 2.2271 - val_loss: 9.7579 - val_mae: 2.4353\n",
            "Epoch 92/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1911 - mae: 2.2283 - val_loss: 9.7603 - val_mae: 2.4370\n",
            "Epoch 93/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1950 - mae: 2.2336 - val_loss: 9.7623 - val_mae: 2.4377\n",
            "Epoch 94/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1976 - mae: 2.2358 - val_loss: 9.7599 - val_mae: 2.4367\n",
            "Epoch 95/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1917 - mae: 2.2316 - val_loss: 9.7662 - val_mae: 2.4389\n",
            "Epoch 96/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1904 - mae: 2.2300 - val_loss: 9.7671 - val_mae: 2.4391\n",
            "Epoch 97/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1852 - mae: 2.2257 - val_loss: 9.7709 - val_mae: 2.4359\n",
            "Epoch 98/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1869 - mae: 2.2282 - val_loss: 9.7774 - val_mae: 2.4379\n",
            "Epoch 99/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1909 - mae: 2.2304 - val_loss: 9.7955 - val_mae: 2.4428\n",
            "Epoch 100/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1920 - mae: 2.2321 - val_loss: 9.7613 - val_mae: 2.4373\n",
            "Epoch 101/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1905 - mae: 2.2319 - val_loss: 9.7569 - val_mae: 2.4358\n",
            "Epoch 102/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1889 - mae: 2.2297 - val_loss: 9.7526 - val_mae: 2.4324\n",
            "Epoch 103/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1872 - mae: 2.2277 - val_loss: 9.7365 - val_mae: 2.4248\n",
            "Epoch 104/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.1922 - mae: 2.2297 - val_loss: 9.7567 - val_mae: 2.4299\n",
            "Epoch 105/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1858 - mae: 2.2270 - val_loss: 9.7380 - val_mae: 2.4254\n",
            "Epoch 106/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1891 - mae: 2.2290 - val_loss: 9.7386 - val_mae: 2.4256\n",
            "Epoch 107/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.1864 - mae: 2.2254 - val_loss: 9.7362 - val_mae: 2.4235\n",
            "Epoch 108/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1888 - mae: 2.2301 - val_loss: 9.7408 - val_mae: 2.4261\n",
            "Epoch 109/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1902 - mae: 2.2299 - val_loss: 9.7386 - val_mae: 2.4256\n",
            "Epoch 110/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1892 - mae: 2.2275 - val_loss: 9.7545 - val_mae: 2.4336\n",
            "Epoch 111/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1934 - mae: 2.2282 - val_loss: 9.7537 - val_mae: 2.4333\n",
            "Epoch 112/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2057 - mae: 2.2367 - val_loss: 9.7533 - val_mae: 2.4332\n",
            "Epoch 113/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.2017 - mae: 2.2346 - val_loss: 9.7546 - val_mae: 2.4337\n",
            "Epoch 114/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1955 - mae: 2.2313 - val_loss: 9.7562 - val_mae: 2.4342\n",
            "Epoch 115/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1932 - mae: 2.2287 - val_loss: 9.7584 - val_mae: 2.4348\n",
            "Epoch 116/200\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 8.1978 - mae: 2.2326 - val_loss: 9.7599 - val_mae: 2.4353\n",
            "Epoch 117/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.2009 - mae: 2.2346 - val_loss: 9.7929 - val_mae: 2.4414\n",
            "Epoch 118/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1987 - mae: 2.2322 - val_loss: 9.7793 - val_mae: 2.4392\n",
            "Epoch 119/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1966 - mae: 2.2328 - val_loss: 9.7859 - val_mae: 2.4384\n",
            "Epoch 120/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 8.2021 - mae: 2.2345 - val_loss: 9.7903 - val_mae: 2.4403\n",
            "Epoch 121/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1974 - mae: 2.2334 - val_loss: 9.7539 - val_mae: 2.4328\n",
            "Epoch 122/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1925 - mae: 2.2293 - val_loss: 9.7726 - val_mae: 2.4374\n",
            "Epoch 123/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1927 - mae: 2.2303 - val_loss: 9.7768 - val_mae: 2.4334\n",
            "Epoch 124/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1895 - mae: 2.2296 - val_loss: 9.7796 - val_mae: 2.4336\n",
            "Epoch 125/200\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 8.1942 - mae: 2.2314 - val_loss: 9.7803 - val_mae: 2.4336\n",
            "Epoch 126/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1865 - mae: 2.2257 - val_loss: 9.7798 - val_mae: 2.4336\n",
            "Epoch 127/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1911 - mae: 2.2315 - val_loss: 9.7604 - val_mae: 2.4329\n",
            "Epoch 128/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1940 - mae: 2.2328 - val_loss: 9.7775 - val_mae: 2.4322\n",
            "Epoch 129/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1911 - mae: 2.2284 - val_loss: 9.7844 - val_mae: 2.4304\n",
            "Epoch 130/200\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 8.1912 - mae: 2.2307 - val_loss: 9.7804 - val_mae: 2.4327\n",
            "Epoch 131/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1880 - mae: 2.2266 - val_loss: 9.7794 - val_mae: 2.4331\n",
            "Epoch 132/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1977 - mae: 2.2347 - val_loss: 9.7798 - val_mae: 2.4337\n",
            "Epoch 133/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1968 - mae: 2.2328 - val_loss: 9.7763 - val_mae: 2.4330\n",
            "Epoch 134/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1874 - mae: 2.2280 - val_loss: 9.7755 - val_mae: 2.4315\n",
            "Epoch 135/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1903 - mae: 2.2280 - val_loss: 9.7555 - val_mae: 2.4345\n",
            "Epoch 136/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1883 - mae: 2.2285 - val_loss: 9.7578 - val_mae: 2.4306\n",
            "Epoch 137/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1911 - mae: 2.2283 - val_loss: 9.7822 - val_mae: 2.4369\n",
            "Epoch 138/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1945 - mae: 2.2336 - val_loss: 9.7861 - val_mae: 2.4374\n",
            "Epoch 139/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1918 - mae: 2.2302 - val_loss: 9.7874 - val_mae: 2.4375\n",
            "Epoch 140/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1920 - mae: 2.2308 - val_loss: 9.7706 - val_mae: 2.4351\n",
            "Epoch 141/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1955 - mae: 2.2328 - val_loss: 9.7747 - val_mae: 2.4335\n",
            "Epoch 142/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1894 - mae: 2.2283 - val_loss: 9.7793 - val_mae: 2.4330\n",
            "Epoch 143/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1867 - mae: 2.2271 - val_loss: 9.7767 - val_mae: 2.4312\n",
            "Epoch 144/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1885 - mae: 2.2291 - val_loss: 9.7786 - val_mae: 2.4330\n",
            "Epoch 145/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1861 - mae: 2.2245 - val_loss: 9.7775 - val_mae: 2.4318\n",
            "Epoch 146/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1850 - mae: 2.2263 - val_loss: 9.7763 - val_mae: 2.4326\n",
            "Epoch 147/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1839 - mae: 2.2240 - val_loss: 9.7785 - val_mae: 2.4312\n",
            "Epoch 148/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1946 - mae: 2.2322 - val_loss: 9.7831 - val_mae: 2.4359\n",
            "Epoch 149/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1895 - mae: 2.2295 - val_loss: 9.7710 - val_mae: 2.4330\n",
            "Epoch 150/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1881 - mae: 2.2272 - val_loss: 9.7401 - val_mae: 2.4244\n",
            "Epoch 151/200\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 8.1926 - mae: 2.2297 - val_loss: 9.7377 - val_mae: 2.4233\n",
            "Epoch 152/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1867 - mae: 2.2282 - val_loss: 9.7539 - val_mae: 2.4282\n",
            "Epoch 153/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1871 - mae: 2.2254 - val_loss: 9.7678 - val_mae: 2.4309\n",
            "Epoch 154/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1908 - mae: 2.2303 - val_loss: 9.7758 - val_mae: 2.4324\n",
            "Epoch 155/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1893 - mae: 2.2280 - val_loss: 9.7767 - val_mae: 2.4316\n",
            "Epoch 156/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1870 - mae: 2.2267 - val_loss: 9.7783 - val_mae: 2.4326\n",
            "Epoch 157/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1908 - mae: 2.2290 - val_loss: 9.7420 - val_mae: 2.4275\n",
            "Epoch 158/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1878 - mae: 2.2262 - val_loss: 9.7459 - val_mae: 2.4266\n",
            "Epoch 159/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1849 - mae: 2.2237 - val_loss: 9.7545 - val_mae: 2.4285\n",
            "Epoch 160/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1890 - mae: 2.2278 - val_loss: 9.7426 - val_mae: 2.4254\n",
            "Epoch 161/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1847 - mae: 2.2254 - val_loss: 9.7701 - val_mae: 2.4299\n",
            "Epoch 162/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1842 - mae: 2.2249 - val_loss: 9.7812 - val_mae: 2.4314\n",
            "Epoch 163/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1848 - mae: 2.2256 - val_loss: 9.7811 - val_mae: 2.4355\n",
            "Epoch 164/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1821 - mae: 2.2210 - val_loss: 9.7751 - val_mae: 2.4325\n",
            "Epoch 165/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1834 - mae: 2.2222 - val_loss: 9.7745 - val_mae: 2.4301\n",
            "Epoch 166/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1901 - mae: 2.2274 - val_loss: 9.7791 - val_mae: 2.4337\n",
            "Epoch 167/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1857 - mae: 2.2269 - val_loss: 9.7816 - val_mae: 2.4365\n",
            "Epoch 168/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1839 - mae: 2.2238 - val_loss: 9.7739 - val_mae: 2.4275\n",
            "Epoch 169/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1803 - mae: 2.2206 - val_loss: 9.7721 - val_mae: 2.4294\n",
            "Epoch 170/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1838 - mae: 2.2222 - val_loss: 9.7461 - val_mae: 2.4254\n",
            "Epoch 171/200\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 8.1809 - mae: 2.2222 - val_loss: 9.7544 - val_mae: 2.4254\n",
            "Epoch 172/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1825 - mae: 2.2214 - val_loss: 9.7393 - val_mae: 2.4218\n",
            "Epoch 173/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1833 - mae: 2.2233 - val_loss: 9.7528 - val_mae: 2.4262\n",
            "Epoch 174/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1838 - mae: 2.2245 - val_loss: 9.7740 - val_mae: 2.4293\n",
            "Epoch 175/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1840 - mae: 2.2250 - val_loss: 9.7817 - val_mae: 2.4348\n",
            "Epoch 176/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1780 - mae: 2.2197 - val_loss: 9.7841 - val_mae: 2.4341\n",
            "Epoch 177/200\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 8.1809 - mae: 2.2222 - val_loss: 9.7426 - val_mae: 2.4227\n",
            "Epoch 178/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1845 - mae: 2.2245 - val_loss: 9.7458 - val_mae: 2.4291\n",
            "Epoch 179/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1842 - mae: 2.2260 - val_loss: 9.7389 - val_mae: 2.4251\n",
            "Epoch 180/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1842 - mae: 2.2232 - val_loss: 9.7468 - val_mae: 2.4252\n",
            "Epoch 181/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1834 - mae: 2.2230 - val_loss: 9.7416 - val_mae: 2.4214\n",
            "Epoch 182/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1817 - mae: 2.2228 - val_loss: 9.7555 - val_mae: 2.4256\n",
            "Epoch 183/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1825 - mae: 2.2242 - val_loss: 9.7483 - val_mae: 2.4242\n",
            "Epoch 184/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1859 - mae: 2.2263 - val_loss: 9.7746 - val_mae: 2.4306\n",
            "Epoch 185/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1849 - mae: 2.2256 - val_loss: 9.7820 - val_mae: 2.4349\n",
            "Epoch 186/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1849 - mae: 2.2254 - val_loss: 9.7748 - val_mae: 2.4309\n",
            "Epoch 187/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1841 - mae: 2.2249 - val_loss: 9.7862 - val_mae: 2.4378\n",
            "Epoch 188/200\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 8.1853 - mae: 2.2255 - val_loss: 9.7777 - val_mae: 2.4333\n",
            "Epoch 189/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1781 - mae: 2.2176 - val_loss: 9.7690 - val_mae: 2.4330\n",
            "Epoch 190/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1783 - mae: 2.2175 - val_loss: 9.7664 - val_mae: 2.4308\n",
            "Epoch 191/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1815 - mae: 2.2226 - val_loss: 9.7547 - val_mae: 2.4299\n",
            "Epoch 192/200\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 8.1884 - mae: 2.2283 - val_loss: 9.7606 - val_mae: 2.4254\n",
            "Epoch 193/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1792 - mae: 2.2186 - val_loss: 9.7742 - val_mae: 2.4304\n",
            "Epoch 194/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1822 - mae: 2.2219 - val_loss: 9.7749 - val_mae: 2.4298\n",
            "Epoch 195/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1856 - mae: 2.2254 - val_loss: 9.7720 - val_mae: 2.4259\n",
            "Epoch 196/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1821 - mae: 2.2220 - val_loss: 9.7736 - val_mae: 2.4280\n",
            "Epoch 197/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1845 - mae: 2.2261 - val_loss: 9.7739 - val_mae: 2.4282\n",
            "Epoch 198/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1828 - mae: 2.2227 - val_loss: 9.7713 - val_mae: 2.4336\n",
            "Epoch 199/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1801 - mae: 2.2212 - val_loss: 9.7828 - val_mae: 2.4317\n",
            "Epoch 200/200\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 8.1834 - mae: 2.2234 - val_loss: 9.7761 - val_mae: 2.4302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTwtTCZlQImP",
        "outputId": "f4133f3b-bc5f-440e-fabf-b6c1332287d6"
      },
      "source": [
        "predictions = model.predict(X_test, batch_size=1, verbose=1)\n",
        "print(predictions)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "229/229 [==============================] - 0s 1ms/step\n",
            "[[-0.9991616 ]\n",
            " [-0.9998793 ]\n",
            " [-0.9998304 ]\n",
            " [-0.9985023 ]\n",
            " [-0.99983335]\n",
            " [-0.99952185]\n",
            " [-0.9998802 ]\n",
            " [-0.9998747 ]\n",
            " [-0.99987966]\n",
            " [-0.9998771 ]\n",
            " [-0.9998782 ]\n",
            " [-0.98370534]\n",
            " [-0.99988   ]\n",
            " [-0.998674  ]\n",
            " [-0.9998455 ]\n",
            " [-0.99976486]\n",
            " [-0.9997455 ]\n",
            " [-0.9997825 ]\n",
            " [-0.9998732 ]\n",
            " [-0.3420575 ]\n",
            " [-0.9998439 ]\n",
            " [-0.99986714]\n",
            " [-0.9996868 ]\n",
            " [-0.99987036]\n",
            " [-0.99986213]\n",
            " [-0.99984866]\n",
            " [-0.9978577 ]\n",
            " [-0.99961156]\n",
            " [-0.9998792 ]\n",
            " [-0.9997185 ]\n",
            " [-0.99987936]\n",
            " [-0.99987876]\n",
            " [-0.9998793 ]\n",
            " [-0.9998593 ]\n",
            " [-0.99970543]\n",
            " [-0.9998792 ]\n",
            " [-0.9998802 ]\n",
            " [-0.99987924]\n",
            " [-0.9998799 ]\n",
            " [-0.99987906]\n",
            " [-0.99986553]\n",
            " [-0.99987084]\n",
            " [-0.99987245]\n",
            " [-0.99971825]\n",
            " [-0.99988025]\n",
            " [-0.99976796]\n",
            " [-0.99987847]\n",
            " [-0.99987936]\n",
            " [-0.9998316 ]\n",
            " [-0.9998601 ]\n",
            " [-0.99986464]\n",
            " [-0.9998761 ]\n",
            " [-0.99981815]\n",
            " [-0.9998649 ]\n",
            " [-0.99988014]\n",
            " [-0.9998657 ]\n",
            " [-0.999879  ]\n",
            " [-0.9996535 ]\n",
            " [ 0.999991  ]\n",
            " [-0.9998799 ]\n",
            " [-0.9998797 ]\n",
            " [-0.99946374]\n",
            " [-0.9998796 ]\n",
            " [-0.99987894]\n",
            " [-0.9998478 ]\n",
            " [-0.9998786 ]\n",
            " [-0.9998459 ]\n",
            " [-0.9998803 ]\n",
            " [-0.9998731 ]\n",
            " [-0.99954545]\n",
            " [-0.9988705 ]\n",
            " [-0.9960961 ]\n",
            " [-0.9924037 ]\n",
            " [-0.99987864]\n",
            " [-0.99977   ]\n",
            " [-0.9998758 ]\n",
            " [-0.9998104 ]\n",
            " [-0.99987316]\n",
            " [-0.9998748 ]\n",
            " [-0.99983466]\n",
            " [-0.99987984]\n",
            " [-0.9998771 ]\n",
            " [ 0.9999985 ]\n",
            " [-0.99987376]\n",
            " [-0.9997228 ]\n",
            " [-0.99852204]\n",
            " [ 0.99995387]\n",
            " [-0.99979305]\n",
            " [-0.9998635 ]\n",
            " [-0.9998797 ]\n",
            " [-0.9995676 ]\n",
            " [-0.99987745]\n",
            " [-0.9988879 ]\n",
            " [-0.9998669 ]\n",
            " [-0.9984568 ]\n",
            " [-0.99977726]\n",
            " [-0.9996074 ]\n",
            " [-0.99940825]\n",
            " [-0.9998794 ]\n",
            " [-0.9998721 ]\n",
            " [-0.99847806]\n",
            " [-0.9998779 ]\n",
            " [-0.99983597]\n",
            " [-0.9998069 ]\n",
            " [-0.99987173]\n",
            " [-0.96505797]\n",
            " [-0.9998792 ]\n",
            " [-0.9996467 ]\n",
            " [-0.99683326]\n",
            " [-0.9994545 ]\n",
            " [-0.99958795]\n",
            " [-0.99987537]\n",
            " [-0.9998714 ]\n",
            " [-0.9994757 ]\n",
            " [-0.99985045]\n",
            " [-0.9998462 ]\n",
            " [-0.9998749 ]\n",
            " [-0.9998675 ]\n",
            " [-0.99977505]\n",
            " [-0.99942887]\n",
            " [-0.9996207 ]\n",
            " [-0.99987096]\n",
            " [-0.99987525]\n",
            " [-0.9998462 ]\n",
            " [-0.9998796 ]\n",
            " [-0.9998462 ]\n",
            " [-0.999875  ]\n",
            " [-0.99988   ]\n",
            " [-0.9998766 ]\n",
            " [-0.99725413]\n",
            " [-0.9997204 ]\n",
            " [-0.9998796 ]\n",
            " [-0.999879  ]\n",
            " [-0.9998687 ]\n",
            " [-0.9998715 ]\n",
            " [-0.9998789 ]\n",
            " [-0.9996971 ]\n",
            " [-0.9997167 ]\n",
            " [-0.99981433]\n",
            " [ 0.99981046]\n",
            " [-0.99978507]\n",
            " [-0.9996416 ]\n",
            " [-0.9991843 ]\n",
            " [-0.9998805 ]\n",
            " [-0.99950963]\n",
            " [-0.9998772 ]\n",
            " [-0.9991081 ]\n",
            " [-0.99987924]\n",
            " [-0.9998777 ]\n",
            " [-0.9998714 ]\n",
            " [-0.9998581 ]\n",
            " [-0.9998796 ]\n",
            " [-0.9998374 ]\n",
            " [-0.9996668 ]\n",
            " [-0.9998024 ]\n",
            " [-0.99987954]\n",
            " [-0.9997374 ]\n",
            " [-0.99973345]\n",
            " [-0.99982685]\n",
            " [-0.99987966]\n",
            " [-0.99961215]\n",
            " [-0.99780613]\n",
            " [-0.99987936]\n",
            " [-0.99987835]\n",
            " [-0.9996503 ]\n",
            " [-0.99977845]\n",
            " [-0.99987674]\n",
            " [-0.9998625 ]\n",
            " [-0.99986535]\n",
            " [-0.9997589 ]\n",
            " [-0.99988014]\n",
            " [-0.9998805 ]\n",
            " [-0.999776  ]\n",
            " [-0.9998793 ]\n",
            " [-0.9996587 ]\n",
            " [-0.9998132 ]\n",
            " [-0.99978447]\n",
            " [-0.99981505]\n",
            " [-0.9998806 ]\n",
            " [-0.9978912 ]\n",
            " [-0.99987674]\n",
            " [-0.99985886]\n",
            " [-0.9998793 ]\n",
            " [-0.9983246 ]\n",
            " [-0.999806  ]\n",
            " [-0.9998789 ]\n",
            " [-0.9998701 ]\n",
            " [-0.9998805 ]\n",
            " [-0.9998801 ]\n",
            " [-0.9998378 ]\n",
            " [-0.99986684]\n",
            " [-0.99987996]\n",
            " [-0.99986774]\n",
            " [-0.9998805 ]\n",
            " [-0.9981874 ]\n",
            " [-0.9998507 ]\n",
            " [-0.16981587]\n",
            " [-0.9998043 ]\n",
            " [-0.9901957 ]\n",
            " [-0.9998774 ]\n",
            " [-0.9965514 ]\n",
            " [-0.6566485 ]\n",
            " [-0.9998728 ]\n",
            " [-0.9998805 ]\n",
            " [-0.99986076]\n",
            " [-0.9998718 ]\n",
            " [-0.99985236]\n",
            " [-0.99783677]\n",
            " [-0.99987996]\n",
            " [-0.9986874 ]\n",
            " [-0.999869  ]\n",
            " [-0.9995975 ]\n",
            " [-0.999878  ]\n",
            " [-0.9925091 ]\n",
            " [-0.99987996]\n",
            " [-0.9998464 ]\n",
            " [-0.99988043]\n",
            " [-0.99986416]\n",
            " [-0.9998712 ]\n",
            " [-0.99985164]\n",
            " [-0.99986774]\n",
            " [-0.99985266]\n",
            " [-0.9998795 ]\n",
            " [-0.99971557]\n",
            " [-0.99987626]\n",
            " [-0.9998801 ]\n",
            " [-0.9982041 ]\n",
            " [-0.9998785 ]\n",
            " [-0.8949501 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqLA_ukHQfTl",
        "outputId": "e3356991-41ef-4b44-8c50-d9671f8605a5"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.9549223046823867"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu1VvuR9W9vY"
      },
      "source": [
        "# general imports\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# keras imports\n",
        "from keras.layers import (Input, Dense, Conv1D, MaxPool1D, Dropout, GRU, LSTM, \n",
        "                          TimeDistributed, Add, Flatten, RepeatVector, Lambda, Concatenate)\n",
        "from keras.models import Model, load_model\n",
        "from keras.metrics import binary_crossentropy\n",
        "from keras import initializers\n",
        "import keras.backend as K\n",
        "\n",
        "# Visualization\n",
        "#from keras_sequential_ascii import keras2ascii\n",
        "\n",
        "# utils functions\n",
        "#from utils import *\n",
        "\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "# Remove warnings from output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}